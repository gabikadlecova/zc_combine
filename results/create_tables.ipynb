{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cfg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_seed</th>\n",
       "      <th>proxy</th>\n",
       "      <th>train_size</th>\n",
       "      <th>use_all_proxies</th>\n",
       "      <th>use_features</th>\n",
       "      <th>use_flops_params</th>\n",
       "      <th>use_onehot</th>\n",
       "      <th>use_path_encoding</th>\n",
       "      <th>features</th>\n",
       "      <th>tau</th>\n",
       "      <th>corr</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614814</td>\n",
       "      <td>0.790337</td>\n",
       "      <td>1.783436</td>\n",
       "      <td>0.033019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614964</td>\n",
       "      <td>0.795266</td>\n",
       "      <td>1.994052</td>\n",
       "      <td>0.054884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.614916</td>\n",
       "      <td>0.789974</td>\n",
       "      <td>1.718632</td>\n",
       "      <td>0.032478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.599426</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>1.721620</td>\n",
       "      <td>0.035014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.603607</td>\n",
       "      <td>0.783472</td>\n",
       "      <td>1.580352</td>\n",
       "      <td>0.030355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>20995</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.595704</td>\n",
       "      <td>0.789150</td>\n",
       "      <td>0.142381</td>\n",
       "      <td>0.020290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>20996</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.622825</td>\n",
       "      <td>0.807945</td>\n",
       "      <td>0.141534</td>\n",
       "      <td>0.019588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>20997</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.625466</td>\n",
       "      <td>0.820530</td>\n",
       "      <td>0.142932</td>\n",
       "      <td>0.019835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>20998</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.590101</td>\n",
       "      <td>0.772822</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>0.020301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>20999</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.511867</td>\n",
       "      <td>0.690840</td>\n",
       "      <td>0.144131</td>\n",
       "      <td>0.019610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                     cfg      dataset  \\\n",
       "0               0  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "1               1  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "2               2  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "3               3  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "4               4  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "...           ...                                     ...          ...   \n",
       "20995       20995  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20996       20996  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20997       20997  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20998       20998  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20999       20999  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "\n",
       "       data_seed  proxy  train_size  use_all_proxies  use_features  \\\n",
       "0           42.0    NaN        1024            False          True   \n",
       "1           43.0    NaN        1024            False          True   \n",
       "2           44.0    NaN        1024            False          True   \n",
       "3           45.0    NaN        1024            False          True   \n",
       "4           46.0    NaN        1024            False          True   \n",
       "...          ...    ...         ...              ...           ...   \n",
       "20995       87.0    NaN          32             True          True   \n",
       "20996       88.0    NaN          32             True          True   \n",
       "20997       89.0    NaN          32             True          True   \n",
       "20998       90.0    NaN          32             True          True   \n",
       "20999       91.0    NaN          32             True          True   \n",
       "\n",
       "      use_flops_params  use_onehot  use_path_encoding  features       tau  \\\n",
       "0                 True       False              False       NaN  0.614814   \n",
       "1                 True       False              False       NaN  0.614964   \n",
       "2                 True       False              False       NaN  0.614916   \n",
       "3                 True       False              False       NaN  0.599426   \n",
       "4                 True       False              False       NaN  0.603607   \n",
       "...                ...         ...                ...       ...       ...   \n",
       "20995              NaN        True               True       NaN  0.595704   \n",
       "20996              NaN        True               True       NaN  0.622825   \n",
       "20997              NaN        True               True       NaN  0.625466   \n",
       "20998              NaN        True               True       NaN  0.590101   \n",
       "20999              NaN        True               True       NaN  0.511867   \n",
       "\n",
       "           corr  fit_time  test_time  \n",
       "0      0.790337  1.783436   0.033019  \n",
       "1      0.795266  1.994052   0.054884  \n",
       "2      0.789974  1.718632   0.032478  \n",
       "3      0.779000  1.721620   0.035014  \n",
       "4      0.783472  1.580352   0.030355  \n",
       "...         ...       ...        ...  \n",
       "20995  0.789150  0.142381   0.020290  \n",
       "20996  0.807945  0.141534   0.019588  \n",
       "20997  0.820530  0.142932   0.019835  \n",
       "20998  0.772822  0.145639   0.020301  \n",
       "20999  0.690840  0.144131   0.019610  \n",
       "\n",
       "[21000 rows x 16 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('tnb101_micro.csv')\n",
    "\n",
    "if 'use_path_encoding' not in data.columns:\n",
    "    data['use_path_encoding']=False\n",
    "data['use_path_encoding'].fillna(False, inplace=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>cfg</th>\n",
       "      <th>dataset</th>\n",
       "      <th>data_seed</th>\n",
       "      <th>proxy</th>\n",
       "      <th>train_size</th>\n",
       "      <th>use_all_proxies</th>\n",
       "      <th>use_features</th>\n",
       "      <th>use_flops_params</th>\n",
       "      <th>use_onehot</th>\n",
       "      <th>use_path_encoding</th>\n",
       "      <th>features</th>\n",
       "      <th>tau</th>\n",
       "      <th>corr</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>test_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>42.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SF</td>\n",
       "      <td>0.614814</td>\n",
       "      <td>0.790337</td>\n",
       "      <td>1.783436</td>\n",
       "      <td>0.033019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SF</td>\n",
       "      <td>0.614964</td>\n",
       "      <td>0.795266</td>\n",
       "      <td>1.994052</td>\n",
       "      <td>0.054884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SF</td>\n",
       "      <td>0.614916</td>\n",
       "      <td>0.789974</td>\n",
       "      <td>1.718632</td>\n",
       "      <td>0.032478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SF</td>\n",
       "      <td>0.599426</td>\n",
       "      <td>0.779000</td>\n",
       "      <td>1.721620</td>\n",
       "      <td>0.035014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>normal</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SF</td>\n",
       "      <td>0.603607</td>\n",
       "      <td>0.783472</td>\n",
       "      <td>1.580352</td>\n",
       "      <td>0.030355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>20995</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PSOFE</td>\n",
       "      <td>0.595704</td>\n",
       "      <td>0.789150</td>\n",
       "      <td>0.142381</td>\n",
       "      <td>0.020290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>20996</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PSOFE</td>\n",
       "      <td>0.622825</td>\n",
       "      <td>0.807945</td>\n",
       "      <td>0.141534</td>\n",
       "      <td>0.019588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>20997</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PSOFE</td>\n",
       "      <td>0.625466</td>\n",
       "      <td>0.820530</td>\n",
       "      <td>0.142932</td>\n",
       "      <td>0.019835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>20998</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PSOFE</td>\n",
       "      <td>0.590101</td>\n",
       "      <td>0.772822</td>\n",
       "      <td>0.145639</td>\n",
       "      <td>0.020301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>20999</td>\n",
       "      <td>../zc_combine/configs/tnb101_full.json</td>\n",
       "      <td>class_scene</td>\n",
       "      <td>91.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PSOFE</td>\n",
       "      <td>0.511867</td>\n",
       "      <td>0.690840</td>\n",
       "      <td>0.144131</td>\n",
       "      <td>0.019610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                     cfg      dataset  \\\n",
       "0               0  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "1               1  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "2               2  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "3               3  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "4               4  ../zc_combine/configs/tnb101_full.json       normal   \n",
       "...           ...                                     ...          ...   \n",
       "20995       20995  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20996       20996  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20997       20997  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20998       20998  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "20999       20999  ../zc_combine/configs/tnb101_full.json  class_scene   \n",
       "\n",
       "       data_seed  proxy  train_size  use_all_proxies  use_features  \\\n",
       "0           42.0    NaN        1024            False          True   \n",
       "1           43.0    NaN        1024            False          True   \n",
       "2           44.0    NaN        1024            False          True   \n",
       "3           45.0    NaN        1024            False          True   \n",
       "4           46.0    NaN        1024            False          True   \n",
       "...          ...    ...         ...              ...           ...   \n",
       "20995       87.0    NaN          32             True          True   \n",
       "20996       88.0    NaN          32             True          True   \n",
       "20997       89.0    NaN          32             True          True   \n",
       "20998       90.0    NaN          32             True          True   \n",
       "20999       91.0    NaN          32             True          True   \n",
       "\n",
       "      use_flops_params  use_onehot  use_path_encoding features       tau  \\\n",
       "0                 True       False              False       SF  0.614814   \n",
       "1                 True       False              False       SF  0.614964   \n",
       "2                 True       False              False       SF  0.614916   \n",
       "3                 True       False              False       SF  0.599426   \n",
       "4                 True       False              False       SF  0.603607   \n",
       "...                ...         ...                ...      ...       ...   \n",
       "20995              NaN        True               True    PSOFE  0.595704   \n",
       "20996              NaN        True               True    PSOFE  0.622825   \n",
       "20997              NaN        True               True    PSOFE  0.625466   \n",
       "20998              NaN        True               True    PSOFE  0.590101   \n",
       "20999              NaN        True               True    PSOFE  0.511867   \n",
       "\n",
       "           corr  fit_time  test_time  \n",
       "0      0.790337  1.783436   0.033019  \n",
       "1      0.795266  1.994052   0.054884  \n",
       "2      0.789974  1.718632   0.032478  \n",
       "3      0.779000  1.721620   0.035014  \n",
       "4      0.783472  1.580352   0.030355  \n",
       "...         ...       ...        ...  \n",
       "20995  0.789150  0.142381   0.020290  \n",
       "20996  0.807945  0.141534   0.019588  \n",
       "20997  0.820530  0.142932   0.019835  \n",
       "20998  0.772822  0.145639   0.020301  \n",
       "20999  0.690840  0.144131   0.019610  \n",
       "\n",
       "[21000 rows x 16 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def feat_string(row):\n",
    "    out = ''\n",
    "    if row['use_all_proxies']:\n",
    "        out += 'P'\n",
    "    if row['use_features']:\n",
    "        out += 'S'\n",
    "    if row['use_onehot']:\n",
    "        out += 'O'\n",
    "    if row['use_flops_params'] or row['use_all_proxies']:\n",
    "        out += 'F'\n",
    "    if row['use_path_encoding']:\n",
    "        out += 'E'\n",
    "    if out == '':\n",
    "        out = 'X'\n",
    "        \n",
    "    return out\n",
    "\n",
    "features_str = {'X': 'None', \n",
    "                'O': 'Onehot', \n",
    "                'S': 'GRAF', \n",
    "                'OF': 'Onehot + F&P', \n",
    "                'SO': 'GRAF + Onehot', \n",
    "                'SF': 'GRAF + F&P', \n",
    "                'SOF': 'GRAF + Onehot + F&P', \n",
    "                'PF': 'ZCP + F&P', \n",
    "                'POF': 'ZCP + Onehot + F&P', \n",
    "                'PSF': 'ZCP + GRAF + F&P', \n",
    "                'PSOF': 'Everything',\n",
    "                'OE': 'Onehot + PE', \n",
    "                'SE': 'GRAF + PE', \n",
    "                'OFE': 'Onehot + F&P + PE', \n",
    "                'SOE': 'GRAF + Onehot + PE', \n",
    "                'SFE': 'GRAF + F&P + PE', \n",
    "                'SOFE': 'GRAF + Onehot + F&P + PE', \n",
    "                'PFE': 'ZCP + F&P + PE', \n",
    "                'POFE': 'ZCP + Onehot + F&P + PE', \n",
    "                'PSFE': 'ZCP + GRAF + F&P + PE', \n",
    "                'PSOFE': 'Everything + PE'\n",
    "               }\n",
    "\n",
    "labels = ['O', 'S', 'OF', 'SO', 'SF', 'SOF', 'PF', 'POF', 'PSF', 'PSOF', 'OE', 'SE', 'OFE', 'SOE', 'SFE', 'SOFE', 'PFE', 'POFE', 'PSFE', 'PSOFE']\n",
    "\n",
    "data['features'] = data.apply(feat_string, axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th colspan=\"10\" halign=\"left\">autoencoder</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">segmentsemantic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>train_size</th>\n",
       "      <th colspan=\"10\" halign=\"left\">32</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">1024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>O</th>\n",
       "      <th>OE</th>\n",
       "      <th>OF</th>\n",
       "      <th>OFE</th>\n",
       "      <th>PF</th>\n",
       "      <th>PFE</th>\n",
       "      <th>POF</th>\n",
       "      <th>POFE</th>\n",
       "      <th>PSF</th>\n",
       "      <th>PSFE</th>\n",
       "      <th>...</th>\n",
       "      <th>PSOF</th>\n",
       "      <th>PSOFE</th>\n",
       "      <th>S</th>\n",
       "      <th>SE</th>\n",
       "      <th>SF</th>\n",
       "      <th>SFE</th>\n",
       "      <th>SO</th>\n",
       "      <th>SOE</th>\n",
       "      <th>SOF</th>\n",
       "      <th>SOFE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>data_seed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"50\" valign=\"top\">tau</th>\n",
       "      <th>42.0</th>\n",
       "      <td>0.448623</td>\n",
       "      <td>0.477173</td>\n",
       "      <td>0.353043</td>\n",
       "      <td>0.476320</td>\n",
       "      <td>0.381997</td>\n",
       "      <td>0.391862</td>\n",
       "      <td>0.414067</td>\n",
       "      <td>0.393243</td>\n",
       "      <td>0.446897</td>\n",
       "      <td>0.435244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812955</td>\n",
       "      <td>0.811572</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.818922</td>\n",
       "      <td>0.814376</td>\n",
       "      <td>0.815722</td>\n",
       "      <td>0.823819</td>\n",
       "      <td>0.823504</td>\n",
       "      <td>0.823493</td>\n",
       "      <td>0.824787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43.0</th>\n",
       "      <td>0.472012</td>\n",
       "      <td>0.505203</td>\n",
       "      <td>0.468237</td>\n",
       "      <td>0.538157</td>\n",
       "      <td>0.485385</td>\n",
       "      <td>0.526505</td>\n",
       "      <td>0.527333</td>\n",
       "      <td>0.528291</td>\n",
       "      <td>0.543472</td>\n",
       "      <td>0.547068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810909</td>\n",
       "      <td>0.811396</td>\n",
       "      <td>0.808229</td>\n",
       "      <td>0.814645</td>\n",
       "      <td>0.806100</td>\n",
       "      <td>0.809826</td>\n",
       "      <td>0.819391</td>\n",
       "      <td>0.821001</td>\n",
       "      <td>0.817118</td>\n",
       "      <td>0.818170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44.0</th>\n",
       "      <td>0.271206</td>\n",
       "      <td>0.271685</td>\n",
       "      <td>0.268277</td>\n",
       "      <td>0.263906</td>\n",
       "      <td>0.476194</td>\n",
       "      <td>0.487454</td>\n",
       "      <td>0.437029</td>\n",
       "      <td>0.459634</td>\n",
       "      <td>0.538023</td>\n",
       "      <td>0.533351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808056</td>\n",
       "      <td>0.810687</td>\n",
       "      <td>0.814685</td>\n",
       "      <td>0.817459</td>\n",
       "      <td>0.808838</td>\n",
       "      <td>0.814163</td>\n",
       "      <td>0.819446</td>\n",
       "      <td>0.821568</td>\n",
       "      <td>0.817803</td>\n",
       "      <td>0.820987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45.0</th>\n",
       "      <td>0.447629</td>\n",
       "      <td>0.503385</td>\n",
       "      <td>0.442303</td>\n",
       "      <td>0.502766</td>\n",
       "      <td>0.439060</td>\n",
       "      <td>0.498798</td>\n",
       "      <td>0.475479</td>\n",
       "      <td>0.498735</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.539651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798316</td>\n",
       "      <td>0.800458</td>\n",
       "      <td>0.806476</td>\n",
       "      <td>0.813537</td>\n",
       "      <td>0.809206</td>\n",
       "      <td>0.814211</td>\n",
       "      <td>0.815500</td>\n",
       "      <td>0.817636</td>\n",
       "      <td>0.816148</td>\n",
       "      <td>0.817711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46.0</th>\n",
       "      <td>0.375814</td>\n",
       "      <td>0.483588</td>\n",
       "      <td>0.397173</td>\n",
       "      <td>0.469317</td>\n",
       "      <td>0.474422</td>\n",
       "      <td>0.563493</td>\n",
       "      <td>0.512148</td>\n",
       "      <td>0.546277</td>\n",
       "      <td>0.565986</td>\n",
       "      <td>0.567577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803855</td>\n",
       "      <td>0.805209</td>\n",
       "      <td>0.803109</td>\n",
       "      <td>0.810104</td>\n",
       "      <td>0.799791</td>\n",
       "      <td>0.803901</td>\n",
       "      <td>0.809391</td>\n",
       "      <td>0.812305</td>\n",
       "      <td>0.809575</td>\n",
       "      <td>0.810918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47.0</th>\n",
       "      <td>0.445734</td>\n",
       "      <td>0.479647</td>\n",
       "      <td>0.460481</td>\n",
       "      <td>0.481279</td>\n",
       "      <td>0.461246</td>\n",
       "      <td>0.506111</td>\n",
       "      <td>0.503733</td>\n",
       "      <td>0.498241</td>\n",
       "      <td>0.532132</td>\n",
       "      <td>0.534352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818755</td>\n",
       "      <td>0.820654</td>\n",
       "      <td>0.818358</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.819459</td>\n",
       "      <td>0.823241</td>\n",
       "      <td>0.825460</td>\n",
       "      <td>0.825105</td>\n",
       "      <td>0.828374</td>\n",
       "      <td>0.827191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48.0</th>\n",
       "      <td>0.485588</td>\n",
       "      <td>0.508801</td>\n",
       "      <td>0.462552</td>\n",
       "      <td>0.511104</td>\n",
       "      <td>0.487819</td>\n",
       "      <td>0.517332</td>\n",
       "      <td>0.519222</td>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.590629</td>\n",
       "      <td>0.577101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812630</td>\n",
       "      <td>0.813743</td>\n",
       "      <td>0.814424</td>\n",
       "      <td>0.819603</td>\n",
       "      <td>0.814962</td>\n",
       "      <td>0.820702</td>\n",
       "      <td>0.820011</td>\n",
       "      <td>0.822850</td>\n",
       "      <td>0.823947</td>\n",
       "      <td>0.826776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49.0</th>\n",
       "      <td>0.268788</td>\n",
       "      <td>0.293499</td>\n",
       "      <td>0.225624</td>\n",
       "      <td>0.291799</td>\n",
       "      <td>0.471358</td>\n",
       "      <td>0.498229</td>\n",
       "      <td>0.489264</td>\n",
       "      <td>0.498192</td>\n",
       "      <td>0.557595</td>\n",
       "      <td>0.535445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809851</td>\n",
       "      <td>0.811654</td>\n",
       "      <td>0.806307</td>\n",
       "      <td>0.810321</td>\n",
       "      <td>0.807013</td>\n",
       "      <td>0.809316</td>\n",
       "      <td>0.816454</td>\n",
       "      <td>0.816214</td>\n",
       "      <td>0.816208</td>\n",
       "      <td>0.816083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50.0</th>\n",
       "      <td>0.422539</td>\n",
       "      <td>0.465783</td>\n",
       "      <td>0.403465</td>\n",
       "      <td>0.464604</td>\n",
       "      <td>0.465615</td>\n",
       "      <td>0.470207</td>\n",
       "      <td>0.480001</td>\n",
       "      <td>0.463224</td>\n",
       "      <td>0.583988</td>\n",
       "      <td>0.591878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802293</td>\n",
       "      <td>0.803525</td>\n",
       "      <td>0.800233</td>\n",
       "      <td>0.803732</td>\n",
       "      <td>0.804336</td>\n",
       "      <td>0.805822</td>\n",
       "      <td>0.811242</td>\n",
       "      <td>0.810887</td>\n",
       "      <td>0.813522</td>\n",
       "      <td>0.813493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51.0</th>\n",
       "      <td>0.428920</td>\n",
       "      <td>0.472319</td>\n",
       "      <td>0.439683</td>\n",
       "      <td>0.470865</td>\n",
       "      <td>0.449012</td>\n",
       "      <td>0.459043</td>\n",
       "      <td>0.456846</td>\n",
       "      <td>0.463156</td>\n",
       "      <td>0.553558</td>\n",
       "      <td>0.554331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815811</td>\n",
       "      <td>0.817167</td>\n",
       "      <td>0.814098</td>\n",
       "      <td>0.816830</td>\n",
       "      <td>0.815345</td>\n",
       "      <td>0.818696</td>\n",
       "      <td>0.827233</td>\n",
       "      <td>0.827269</td>\n",
       "      <td>0.824106</td>\n",
       "      <td>0.824842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52.0</th>\n",
       "      <td>0.289672</td>\n",
       "      <td>0.333714</td>\n",
       "      <td>0.288954</td>\n",
       "      <td>0.379036</td>\n",
       "      <td>0.464840</td>\n",
       "      <td>0.503392</td>\n",
       "      <td>0.451444</td>\n",
       "      <td>0.492122</td>\n",
       "      <td>0.553906</td>\n",
       "      <td>0.563883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.788301</td>\n",
       "      <td>0.790610</td>\n",
       "      <td>0.798475</td>\n",
       "      <td>0.796740</td>\n",
       "      <td>0.798521</td>\n",
       "      <td>0.797968</td>\n",
       "      <td>0.804283</td>\n",
       "      <td>0.800831</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.802319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53.0</th>\n",
       "      <td>0.408658</td>\n",
       "      <td>0.363922</td>\n",
       "      <td>0.384089</td>\n",
       "      <td>0.383768</td>\n",
       "      <td>0.495251</td>\n",
       "      <td>0.534231</td>\n",
       "      <td>0.508030</td>\n",
       "      <td>0.495445</td>\n",
       "      <td>0.583055</td>\n",
       "      <td>0.582124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802723</td>\n",
       "      <td>0.803777</td>\n",
       "      <td>0.807100</td>\n",
       "      <td>0.811141</td>\n",
       "      <td>0.805158</td>\n",
       "      <td>0.808840</td>\n",
       "      <td>0.816396</td>\n",
       "      <td>0.817010</td>\n",
       "      <td>0.815505</td>\n",
       "      <td>0.817706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54.0</th>\n",
       "      <td>0.394507</td>\n",
       "      <td>0.415821</td>\n",
       "      <td>0.365229</td>\n",
       "      <td>0.413066</td>\n",
       "      <td>0.515140</td>\n",
       "      <td>0.556148</td>\n",
       "      <td>0.542184</td>\n",
       "      <td>0.555053</td>\n",
       "      <td>0.597861</td>\n",
       "      <td>0.595361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805851</td>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.811195</td>\n",
       "      <td>0.816502</td>\n",
       "      <td>0.807394</td>\n",
       "      <td>0.812652</td>\n",
       "      <td>0.817442</td>\n",
       "      <td>0.820401</td>\n",
       "      <td>0.816907</td>\n",
       "      <td>0.819137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55.0</th>\n",
       "      <td>0.372250</td>\n",
       "      <td>0.471655</td>\n",
       "      <td>0.382377</td>\n",
       "      <td>0.485190</td>\n",
       "      <td>0.409469</td>\n",
       "      <td>0.497607</td>\n",
       "      <td>0.450997</td>\n",
       "      <td>0.493886</td>\n",
       "      <td>0.549892</td>\n",
       "      <td>0.547261</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807429</td>\n",
       "      <td>0.808605</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.814411</td>\n",
       "      <td>0.807321</td>\n",
       "      <td>0.814029</td>\n",
       "      <td>0.817386</td>\n",
       "      <td>0.817833</td>\n",
       "      <td>0.817889</td>\n",
       "      <td>0.818641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56.0</th>\n",
       "      <td>0.450867</td>\n",
       "      <td>0.533203</td>\n",
       "      <td>0.460649</td>\n",
       "      <td>0.528068</td>\n",
       "      <td>0.432870</td>\n",
       "      <td>0.500047</td>\n",
       "      <td>0.524650</td>\n",
       "      <td>0.511771</td>\n",
       "      <td>0.562862</td>\n",
       "      <td>0.570370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815057</td>\n",
       "      <td>0.817170</td>\n",
       "      <td>0.811867</td>\n",
       "      <td>0.815758</td>\n",
       "      <td>0.813700</td>\n",
       "      <td>0.818568</td>\n",
       "      <td>0.823398</td>\n",
       "      <td>0.824427</td>\n",
       "      <td>0.825504</td>\n",
       "      <td>0.826927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57.0</th>\n",
       "      <td>0.522490</td>\n",
       "      <td>0.467214</td>\n",
       "      <td>0.489243</td>\n",
       "      <td>0.486342</td>\n",
       "      <td>0.469642</td>\n",
       "      <td>0.534365</td>\n",
       "      <td>0.531954</td>\n",
       "      <td>0.539248</td>\n",
       "      <td>0.543344</td>\n",
       "      <td>0.532582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804742</td>\n",
       "      <td>0.806516</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.809158</td>\n",
       "      <td>0.805732</td>\n",
       "      <td>0.810079</td>\n",
       "      <td>0.810383</td>\n",
       "      <td>0.810715</td>\n",
       "      <td>0.812978</td>\n",
       "      <td>0.814246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58.0</th>\n",
       "      <td>0.428265</td>\n",
       "      <td>0.481899</td>\n",
       "      <td>0.360731</td>\n",
       "      <td>0.455341</td>\n",
       "      <td>0.464040</td>\n",
       "      <td>0.470022</td>\n",
       "      <td>0.471191</td>\n",
       "      <td>0.473354</td>\n",
       "      <td>0.571521</td>\n",
       "      <td>0.576930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798551</td>\n",
       "      <td>0.799235</td>\n",
       "      <td>0.812031</td>\n",
       "      <td>0.814649</td>\n",
       "      <td>0.810027</td>\n",
       "      <td>0.812996</td>\n",
       "      <td>0.818680</td>\n",
       "      <td>0.819159</td>\n",
       "      <td>0.817263</td>\n",
       "      <td>0.818387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59.0</th>\n",
       "      <td>0.425697</td>\n",
       "      <td>0.443003</td>\n",
       "      <td>0.345634</td>\n",
       "      <td>0.338100</td>\n",
       "      <td>0.487004</td>\n",
       "      <td>0.475806</td>\n",
       "      <td>0.484121</td>\n",
       "      <td>0.471307</td>\n",
       "      <td>0.559728</td>\n",
       "      <td>0.560367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817548</td>\n",
       "      <td>0.818681</td>\n",
       "      <td>0.822502</td>\n",
       "      <td>0.828222</td>\n",
       "      <td>0.823398</td>\n",
       "      <td>0.827337</td>\n",
       "      <td>0.828428</td>\n",
       "      <td>0.830034</td>\n",
       "      <td>0.828108</td>\n",
       "      <td>0.830616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60.0</th>\n",
       "      <td>0.428545</td>\n",
       "      <td>0.485560</td>\n",
       "      <td>0.367866</td>\n",
       "      <td>0.438706</td>\n",
       "      <td>0.508617</td>\n",
       "      <td>0.521912</td>\n",
       "      <td>0.496805</td>\n",
       "      <td>0.530486</td>\n",
       "      <td>0.613104</td>\n",
       "      <td>0.612964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808595</td>\n",
       "      <td>0.811187</td>\n",
       "      <td>0.811194</td>\n",
       "      <td>0.817273</td>\n",
       "      <td>0.811039</td>\n",
       "      <td>0.815452</td>\n",
       "      <td>0.814387</td>\n",
       "      <td>0.816131</td>\n",
       "      <td>0.814708</td>\n",
       "      <td>0.815287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61.0</th>\n",
       "      <td>0.443112</td>\n",
       "      <td>0.537733</td>\n",
       "      <td>0.397926</td>\n",
       "      <td>0.535974</td>\n",
       "      <td>0.469231</td>\n",
       "      <td>0.523646</td>\n",
       "      <td>0.492391</td>\n",
       "      <td>0.533480</td>\n",
       "      <td>0.562994</td>\n",
       "      <td>0.568278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809545</td>\n",
       "      <td>0.810439</td>\n",
       "      <td>0.812523</td>\n",
       "      <td>0.813721</td>\n",
       "      <td>0.810177</td>\n",
       "      <td>0.814102</td>\n",
       "      <td>0.818825</td>\n",
       "      <td>0.819046</td>\n",
       "      <td>0.818950</td>\n",
       "      <td>0.818900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62.0</th>\n",
       "      <td>0.411753</td>\n",
       "      <td>0.480334</td>\n",
       "      <td>0.396796</td>\n",
       "      <td>0.478317</td>\n",
       "      <td>0.478693</td>\n",
       "      <td>0.550527</td>\n",
       "      <td>0.507676</td>\n",
       "      <td>0.548775</td>\n",
       "      <td>0.571787</td>\n",
       "      <td>0.569913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.813050</td>\n",
       "      <td>0.813464</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.804914</td>\n",
       "      <td>0.802510</td>\n",
       "      <td>0.805516</td>\n",
       "      <td>0.811581</td>\n",
       "      <td>0.810964</td>\n",
       "      <td>0.813052</td>\n",
       "      <td>0.812833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63.0</th>\n",
       "      <td>0.392741</td>\n",
       "      <td>0.438965</td>\n",
       "      <td>0.387190</td>\n",
       "      <td>0.434401</td>\n",
       "      <td>0.404633</td>\n",
       "      <td>0.477792</td>\n",
       "      <td>0.480202</td>\n",
       "      <td>0.476970</td>\n",
       "      <td>0.518887</td>\n",
       "      <td>0.506672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802310</td>\n",
       "      <td>0.805129</td>\n",
       "      <td>0.804961</td>\n",
       "      <td>0.806758</td>\n",
       "      <td>0.806755</td>\n",
       "      <td>0.810059</td>\n",
       "      <td>0.814292</td>\n",
       "      <td>0.816105</td>\n",
       "      <td>0.817146</td>\n",
       "      <td>0.817508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64.0</th>\n",
       "      <td>0.462021</td>\n",
       "      <td>0.484946</td>\n",
       "      <td>0.423946</td>\n",
       "      <td>0.480213</td>\n",
       "      <td>0.458718</td>\n",
       "      <td>0.485950</td>\n",
       "      <td>0.456959</td>\n",
       "      <td>0.474778</td>\n",
       "      <td>0.555447</td>\n",
       "      <td>0.552363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811390</td>\n",
       "      <td>0.813269</td>\n",
       "      <td>0.812237</td>\n",
       "      <td>0.815649</td>\n",
       "      <td>0.812730</td>\n",
       "      <td>0.815218</td>\n",
       "      <td>0.821925</td>\n",
       "      <td>0.823148</td>\n",
       "      <td>0.822763</td>\n",
       "      <td>0.823785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65.0</th>\n",
       "      <td>0.384998</td>\n",
       "      <td>0.484273</td>\n",
       "      <td>0.376069</td>\n",
       "      <td>0.473330</td>\n",
       "      <td>0.483462</td>\n",
       "      <td>0.505638</td>\n",
       "      <td>0.504413</td>\n",
       "      <td>0.507473</td>\n",
       "      <td>0.560316</td>\n",
       "      <td>0.571772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820872</td>\n",
       "      <td>0.819922</td>\n",
       "      <td>0.823317</td>\n",
       "      <td>0.824160</td>\n",
       "      <td>0.822256</td>\n",
       "      <td>0.824262</td>\n",
       "      <td>0.830681</td>\n",
       "      <td>0.829854</td>\n",
       "      <td>0.830658</td>\n",
       "      <td>0.830044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66.0</th>\n",
       "      <td>0.472467</td>\n",
       "      <td>0.515086</td>\n",
       "      <td>0.398503</td>\n",
       "      <td>0.511535</td>\n",
       "      <td>0.401946</td>\n",
       "      <td>0.459444</td>\n",
       "      <td>0.446898</td>\n",
       "      <td>0.459628</td>\n",
       "      <td>0.524026</td>\n",
       "      <td>0.547135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816906</td>\n",
       "      <td>0.817409</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.822902</td>\n",
       "      <td>0.816464</td>\n",
       "      <td>0.820933</td>\n",
       "      <td>0.823339</td>\n",
       "      <td>0.825014</td>\n",
       "      <td>0.821768</td>\n",
       "      <td>0.824262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67.0</th>\n",
       "      <td>0.444525</td>\n",
       "      <td>0.488841</td>\n",
       "      <td>0.414096</td>\n",
       "      <td>0.472164</td>\n",
       "      <td>0.471776</td>\n",
       "      <td>0.512027</td>\n",
       "      <td>0.501978</td>\n",
       "      <td>0.516334</td>\n",
       "      <td>0.520650</td>\n",
       "      <td>0.542353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802359</td>\n",
       "      <td>0.804147</td>\n",
       "      <td>0.810974</td>\n",
       "      <td>0.813978</td>\n",
       "      <td>0.813238</td>\n",
       "      <td>0.814522</td>\n",
       "      <td>0.819978</td>\n",
       "      <td>0.821315</td>\n",
       "      <td>0.819505</td>\n",
       "      <td>0.820510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68.0</th>\n",
       "      <td>0.386178</td>\n",
       "      <td>0.370519</td>\n",
       "      <td>0.401951</td>\n",
       "      <td>0.364592</td>\n",
       "      <td>0.499287</td>\n",
       "      <td>0.488592</td>\n",
       "      <td>0.492998</td>\n",
       "      <td>0.480144</td>\n",
       "      <td>0.521859</td>\n",
       "      <td>0.527602</td>\n",
       "      <td>...</td>\n",
       "      <td>0.798427</td>\n",
       "      <td>0.800845</td>\n",
       "      <td>0.801646</td>\n",
       "      <td>0.803236</td>\n",
       "      <td>0.799208</td>\n",
       "      <td>0.801154</td>\n",
       "      <td>0.808956</td>\n",
       "      <td>0.806913</td>\n",
       "      <td>0.807767</td>\n",
       "      <td>0.805750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69.0</th>\n",
       "      <td>0.444872</td>\n",
       "      <td>0.461017</td>\n",
       "      <td>0.336292</td>\n",
       "      <td>0.409555</td>\n",
       "      <td>0.477272</td>\n",
       "      <td>0.535823</td>\n",
       "      <td>0.507645</td>\n",
       "      <td>0.523621</td>\n",
       "      <td>0.568651</td>\n",
       "      <td>0.566717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807717</td>\n",
       "      <td>0.808059</td>\n",
       "      <td>0.816316</td>\n",
       "      <td>0.820005</td>\n",
       "      <td>0.816136</td>\n",
       "      <td>0.819755</td>\n",
       "      <td>0.822270</td>\n",
       "      <td>0.823716</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.825138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70.0</th>\n",
       "      <td>0.406763</td>\n",
       "      <td>0.420498</td>\n",
       "      <td>0.414044</td>\n",
       "      <td>0.420534</td>\n",
       "      <td>0.394608</td>\n",
       "      <td>0.471671</td>\n",
       "      <td>0.450742</td>\n",
       "      <td>0.456360</td>\n",
       "      <td>0.557655</td>\n",
       "      <td>0.558694</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812347</td>\n",
       "      <td>0.814946</td>\n",
       "      <td>0.813572</td>\n",
       "      <td>0.816447</td>\n",
       "      <td>0.812567</td>\n",
       "      <td>0.816293</td>\n",
       "      <td>0.820514</td>\n",
       "      <td>0.822170</td>\n",
       "      <td>0.822620</td>\n",
       "      <td>0.824377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71.0</th>\n",
       "      <td>0.415728</td>\n",
       "      <td>0.487428</td>\n",
       "      <td>0.425795</td>\n",
       "      <td>0.486679</td>\n",
       "      <td>0.452769</td>\n",
       "      <td>0.472229</td>\n",
       "      <td>0.456261</td>\n",
       "      <td>0.461682</td>\n",
       "      <td>0.577379</td>\n",
       "      <td>0.571362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805774</td>\n",
       "      <td>0.807559</td>\n",
       "      <td>0.815014</td>\n",
       "      <td>0.816265</td>\n",
       "      <td>0.811232</td>\n",
       "      <td>0.813476</td>\n",
       "      <td>0.820677</td>\n",
       "      <td>0.820811</td>\n",
       "      <td>0.817943</td>\n",
       "      <td>0.818614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72.0</th>\n",
       "      <td>0.442808</td>\n",
       "      <td>0.504454</td>\n",
       "      <td>0.431128</td>\n",
       "      <td>0.496055</td>\n",
       "      <td>0.434589</td>\n",
       "      <td>0.512512</td>\n",
       "      <td>0.496525</td>\n",
       "      <td>0.519361</td>\n",
       "      <td>0.566281</td>\n",
       "      <td>0.556255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809388</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>0.816764</td>\n",
       "      <td>0.820932</td>\n",
       "      <td>0.811212</td>\n",
       "      <td>0.816189</td>\n",
       "      <td>0.824368</td>\n",
       "      <td>0.826224</td>\n",
       "      <td>0.819033</td>\n",
       "      <td>0.820626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73.0</th>\n",
       "      <td>0.451600</td>\n",
       "      <td>0.531204</td>\n",
       "      <td>0.459936</td>\n",
       "      <td>0.543633</td>\n",
       "      <td>0.447364</td>\n",
       "      <td>0.543243</td>\n",
       "      <td>0.503990</td>\n",
       "      <td>0.542657</td>\n",
       "      <td>0.564631</td>\n",
       "      <td>0.572310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820304</td>\n",
       "      <td>0.820210</td>\n",
       "      <td>0.814263</td>\n",
       "      <td>0.818977</td>\n",
       "      <td>0.815084</td>\n",
       "      <td>0.821279</td>\n",
       "      <td>0.824571</td>\n",
       "      <td>0.826311</td>\n",
       "      <td>0.828499</td>\n",
       "      <td>0.829574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74.0</th>\n",
       "      <td>0.418354</td>\n",
       "      <td>0.444321</td>\n",
       "      <td>0.420425</td>\n",
       "      <td>0.445109</td>\n",
       "      <td>0.456053</td>\n",
       "      <td>0.473899</td>\n",
       "      <td>0.460151</td>\n",
       "      <td>0.467178</td>\n",
       "      <td>0.561458</td>\n",
       "      <td>0.558004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.804328</td>\n",
       "      <td>0.805103</td>\n",
       "      <td>0.807979</td>\n",
       "      <td>0.814011</td>\n",
       "      <td>0.810260</td>\n",
       "      <td>0.818037</td>\n",
       "      <td>0.818267</td>\n",
       "      <td>0.820209</td>\n",
       "      <td>0.815793</td>\n",
       "      <td>0.818536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75.0</th>\n",
       "      <td>0.324110</td>\n",
       "      <td>0.424198</td>\n",
       "      <td>0.236731</td>\n",
       "      <td>0.407496</td>\n",
       "      <td>0.343313</td>\n",
       "      <td>0.342349</td>\n",
       "      <td>0.324510</td>\n",
       "      <td>0.332467</td>\n",
       "      <td>0.549908</td>\n",
       "      <td>0.523725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805837</td>\n",
       "      <td>0.807930</td>\n",
       "      <td>0.807651</td>\n",
       "      <td>0.813818</td>\n",
       "      <td>0.808418</td>\n",
       "      <td>0.812921</td>\n",
       "      <td>0.815190</td>\n",
       "      <td>0.817270</td>\n",
       "      <td>0.815524</td>\n",
       "      <td>0.818626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76.0</th>\n",
       "      <td>0.382361</td>\n",
       "      <td>0.445684</td>\n",
       "      <td>0.361471</td>\n",
       "      <td>0.430837</td>\n",
       "      <td>0.441623</td>\n",
       "      <td>0.473105</td>\n",
       "      <td>0.461089</td>\n",
       "      <td>0.462171</td>\n",
       "      <td>0.536352</td>\n",
       "      <td>0.528878</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803606</td>\n",
       "      <td>0.805183</td>\n",
       "      <td>0.810403</td>\n",
       "      <td>0.813860</td>\n",
       "      <td>0.809542</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.818077</td>\n",
       "      <td>0.818412</td>\n",
       "      <td>0.820285</td>\n",
       "      <td>0.820777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77.0</th>\n",
       "      <td>0.445548</td>\n",
       "      <td>0.509010</td>\n",
       "      <td>0.324138</td>\n",
       "      <td>0.421720</td>\n",
       "      <td>0.517330</td>\n",
       "      <td>0.557018</td>\n",
       "      <td>0.540003</td>\n",
       "      <td>0.559816</td>\n",
       "      <td>0.603023</td>\n",
       "      <td>0.606938</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808206</td>\n",
       "      <td>0.809694</td>\n",
       "      <td>0.812692</td>\n",
       "      <td>0.815559</td>\n",
       "      <td>0.811178</td>\n",
       "      <td>0.813078</td>\n",
       "      <td>0.823485</td>\n",
       "      <td>0.824389</td>\n",
       "      <td>0.822401</td>\n",
       "      <td>0.823679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78.0</th>\n",
       "      <td>0.437315</td>\n",
       "      <td>0.480847</td>\n",
       "      <td>0.329985</td>\n",
       "      <td>0.377106</td>\n",
       "      <td>0.433371</td>\n",
       "      <td>0.450538</td>\n",
       "      <td>0.455683</td>\n",
       "      <td>0.464082</td>\n",
       "      <td>0.546695</td>\n",
       "      <td>0.544781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808698</td>\n",
       "      <td>0.807394</td>\n",
       "      <td>0.807814</td>\n",
       "      <td>0.810363</td>\n",
       "      <td>0.809338</td>\n",
       "      <td>0.810898</td>\n",
       "      <td>0.819703</td>\n",
       "      <td>0.819592</td>\n",
       "      <td>0.819999</td>\n",
       "      <td>0.821310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79.0</th>\n",
       "      <td>0.449707</td>\n",
       "      <td>0.488906</td>\n",
       "      <td>0.441434</td>\n",
       "      <td>0.487836</td>\n",
       "      <td>0.376769</td>\n",
       "      <td>0.424680</td>\n",
       "      <td>0.420066</td>\n",
       "      <td>0.416980</td>\n",
       "      <td>0.437145</td>\n",
       "      <td>0.457966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808404</td>\n",
       "      <td>0.810063</td>\n",
       "      <td>0.812399</td>\n",
       "      <td>0.815306</td>\n",
       "      <td>0.810586</td>\n",
       "      <td>0.814999</td>\n",
       "      <td>0.818756</td>\n",
       "      <td>0.818739</td>\n",
       "      <td>0.818378</td>\n",
       "      <td>0.820412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80.0</th>\n",
       "      <td>0.430740</td>\n",
       "      <td>0.533332</td>\n",
       "      <td>0.459228</td>\n",
       "      <td>0.537714</td>\n",
       "      <td>0.427250</td>\n",
       "      <td>0.445980</td>\n",
       "      <td>0.425059</td>\n",
       "      <td>0.436164</td>\n",
       "      <td>0.506193</td>\n",
       "      <td>0.513135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809383</td>\n",
       "      <td>0.809929</td>\n",
       "      <td>0.804570</td>\n",
       "      <td>0.812018</td>\n",
       "      <td>0.809386</td>\n",
       "      <td>0.815653</td>\n",
       "      <td>0.812754</td>\n",
       "      <td>0.815720</td>\n",
       "      <td>0.817320</td>\n",
       "      <td>0.819245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81.0</th>\n",
       "      <td>0.405685</td>\n",
       "      <td>0.380666</td>\n",
       "      <td>0.405428</td>\n",
       "      <td>0.410265</td>\n",
       "      <td>0.468343</td>\n",
       "      <td>0.512032</td>\n",
       "      <td>0.486535</td>\n",
       "      <td>0.488639</td>\n",
       "      <td>0.557980</td>\n",
       "      <td>0.553580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.810023</td>\n",
       "      <td>0.811288</td>\n",
       "      <td>0.816905</td>\n",
       "      <td>0.819933</td>\n",
       "      <td>0.817992</td>\n",
       "      <td>0.821177</td>\n",
       "      <td>0.822001</td>\n",
       "      <td>0.822963</td>\n",
       "      <td>0.824974</td>\n",
       "      <td>0.826532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82.0</th>\n",
       "      <td>0.282880</td>\n",
       "      <td>0.411413</td>\n",
       "      <td>0.211684</td>\n",
       "      <td>0.345705</td>\n",
       "      <td>0.456484</td>\n",
       "      <td>0.507412</td>\n",
       "      <td>0.475760</td>\n",
       "      <td>0.499485</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.558238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811192</td>\n",
       "      <td>0.811876</td>\n",
       "      <td>0.815031</td>\n",
       "      <td>0.820102</td>\n",
       "      <td>0.813224</td>\n",
       "      <td>0.820246</td>\n",
       "      <td>0.822092</td>\n",
       "      <td>0.822546</td>\n",
       "      <td>0.821560</td>\n",
       "      <td>0.824970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83.0</th>\n",
       "      <td>0.436591</td>\n",
       "      <td>0.503496</td>\n",
       "      <td>0.429974</td>\n",
       "      <td>0.500843</td>\n",
       "      <td>0.461493</td>\n",
       "      <td>0.527037</td>\n",
       "      <td>0.510162</td>\n",
       "      <td>0.532898</td>\n",
       "      <td>0.576101</td>\n",
       "      <td>0.573442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.809842</td>\n",
       "      <td>0.811600</td>\n",
       "      <td>0.811533</td>\n",
       "      <td>0.816673</td>\n",
       "      <td>0.811024</td>\n",
       "      <td>0.816309</td>\n",
       "      <td>0.819257</td>\n",
       "      <td>0.819629</td>\n",
       "      <td>0.820161</td>\n",
       "      <td>0.819977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84.0</th>\n",
       "      <td>0.418366</td>\n",
       "      <td>0.419744</td>\n",
       "      <td>0.412889</td>\n",
       "      <td>0.399055</td>\n",
       "      <td>0.511221</td>\n",
       "      <td>0.549968</td>\n",
       "      <td>0.551914</td>\n",
       "      <td>0.550263</td>\n",
       "      <td>0.606836</td>\n",
       "      <td>0.603732</td>\n",
       "      <td>...</td>\n",
       "      <td>0.803018</td>\n",
       "      <td>0.805110</td>\n",
       "      <td>0.804852</td>\n",
       "      <td>0.807378</td>\n",
       "      <td>0.804385</td>\n",
       "      <td>0.805955</td>\n",
       "      <td>0.808862</td>\n",
       "      <td>0.808655</td>\n",
       "      <td>0.810547</td>\n",
       "      <td>0.810691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85.0</th>\n",
       "      <td>0.432117</td>\n",
       "      <td>0.451533</td>\n",
       "      <td>0.410348</td>\n",
       "      <td>0.378872</td>\n",
       "      <td>0.450651</td>\n",
       "      <td>0.446998</td>\n",
       "      <td>0.453668</td>\n",
       "      <td>0.462816</td>\n",
       "      <td>0.505245</td>\n",
       "      <td>0.501486</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800670</td>\n",
       "      <td>0.802204</td>\n",
       "      <td>0.802057</td>\n",
       "      <td>0.807284</td>\n",
       "      <td>0.804990</td>\n",
       "      <td>0.806574</td>\n",
       "      <td>0.809116</td>\n",
       "      <td>0.810020</td>\n",
       "      <td>0.809635</td>\n",
       "      <td>0.811281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86.0</th>\n",
       "      <td>0.476828</td>\n",
       "      <td>0.468607</td>\n",
       "      <td>0.414228</td>\n",
       "      <td>0.399621</td>\n",
       "      <td>0.464082</td>\n",
       "      <td>0.477902</td>\n",
       "      <td>0.480698</td>\n",
       "      <td>0.474828</td>\n",
       "      <td>0.547670</td>\n",
       "      <td>0.542176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821147</td>\n",
       "      <td>0.819938</td>\n",
       "      <td>0.815950</td>\n",
       "      <td>0.820419</td>\n",
       "      <td>0.817476</td>\n",
       "      <td>0.820345</td>\n",
       "      <td>0.825251</td>\n",
       "      <td>0.826909</td>\n",
       "      <td>0.827867</td>\n",
       "      <td>0.827881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87.0</th>\n",
       "      <td>0.524748</td>\n",
       "      <td>0.538720</td>\n",
       "      <td>0.471599</td>\n",
       "      <td>0.526151</td>\n",
       "      <td>0.450251</td>\n",
       "      <td>0.465265</td>\n",
       "      <td>0.461750</td>\n",
       "      <td>0.470355</td>\n",
       "      <td>0.542003</td>\n",
       "      <td>0.545689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811290</td>\n",
       "      <td>0.812745</td>\n",
       "      <td>0.810451</td>\n",
       "      <td>0.812095</td>\n",
       "      <td>0.809605</td>\n",
       "      <td>0.813258</td>\n",
       "      <td>0.815255</td>\n",
       "      <td>0.816461</td>\n",
       "      <td>0.817617</td>\n",
       "      <td>0.818534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88.0</th>\n",
       "      <td>0.449297</td>\n",
       "      <td>0.543471</td>\n",
       "      <td>0.365648</td>\n",
       "      <td>0.504055</td>\n",
       "      <td>0.486587</td>\n",
       "      <td>0.542846</td>\n",
       "      <td>0.497457</td>\n",
       "      <td>0.543542</td>\n",
       "      <td>0.565296</td>\n",
       "      <td>0.562801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799745</td>\n",
       "      <td>0.801181</td>\n",
       "      <td>0.801443</td>\n",
       "      <td>0.802680</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.798715</td>\n",
       "      <td>0.812282</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.809943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89.0</th>\n",
       "      <td>0.459170</td>\n",
       "      <td>0.534882</td>\n",
       "      <td>0.471245</td>\n",
       "      <td>0.547434</td>\n",
       "      <td>0.529792</td>\n",
       "      <td>0.562644</td>\n",
       "      <td>0.564687</td>\n",
       "      <td>0.562938</td>\n",
       "      <td>0.583568</td>\n",
       "      <td>0.576327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802271</td>\n",
       "      <td>0.801444</td>\n",
       "      <td>0.802982</td>\n",
       "      <td>0.806235</td>\n",
       "      <td>0.806868</td>\n",
       "      <td>0.809044</td>\n",
       "      <td>0.811141</td>\n",
       "      <td>0.810632</td>\n",
       "      <td>0.814105</td>\n",
       "      <td>0.814768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90.0</th>\n",
       "      <td>0.503354</td>\n",
       "      <td>0.524939</td>\n",
       "      <td>0.496997</td>\n",
       "      <td>0.537740</td>\n",
       "      <td>0.453807</td>\n",
       "      <td>0.458390</td>\n",
       "      <td>0.462958</td>\n",
       "      <td>0.456944</td>\n",
       "      <td>0.525057</td>\n",
       "      <td>0.523074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.818098</td>\n",
       "      <td>0.818246</td>\n",
       "      <td>0.814838</td>\n",
       "      <td>0.816703</td>\n",
       "      <td>0.817758</td>\n",
       "      <td>0.819857</td>\n",
       "      <td>0.821587</td>\n",
       "      <td>0.824053</td>\n",
       "      <td>0.824475</td>\n",
       "      <td>0.826558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91.0</th>\n",
       "      <td>0.441129</td>\n",
       "      <td>0.520006</td>\n",
       "      <td>0.424023</td>\n",
       "      <td>0.538422</td>\n",
       "      <td>0.509335</td>\n",
       "      <td>0.575805</td>\n",
       "      <td>0.573798</td>\n",
       "      <td>0.577690</td>\n",
       "      <td>0.588576</td>\n",
       "      <td>0.569913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806699</td>\n",
       "      <td>0.809392</td>\n",
       "      <td>0.810504</td>\n",
       "      <td>0.816423</td>\n",
       "      <td>0.811094</td>\n",
       "      <td>0.815139</td>\n",
       "      <td>0.818213</td>\n",
       "      <td>0.819665</td>\n",
       "      <td>0.821018</td>\n",
       "      <td>0.820283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 420 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "dataset       autoencoder                                                    \\\n",
       "train_size           32                                                       \n",
       "features                O        OE        OF       OFE        PF       PFE   \n",
       "    data_seed                                                                 \n",
       "tau 42.0         0.448623  0.477173  0.353043  0.476320  0.381997  0.391862   \n",
       "    43.0         0.472012  0.505203  0.468237  0.538157  0.485385  0.526505   \n",
       "    44.0         0.271206  0.271685  0.268277  0.263906  0.476194  0.487454   \n",
       "    45.0         0.447629  0.503385  0.442303  0.502766  0.439060  0.498798   \n",
       "    46.0         0.375814  0.483588  0.397173  0.469317  0.474422  0.563493   \n",
       "    47.0         0.445734  0.479647  0.460481  0.481279  0.461246  0.506111   \n",
       "    48.0         0.485588  0.508801  0.462552  0.511104  0.487819  0.517332   \n",
       "    49.0         0.268788  0.293499  0.225624  0.291799  0.471358  0.498229   \n",
       "    50.0         0.422539  0.465783  0.403465  0.464604  0.465615  0.470207   \n",
       "    51.0         0.428920  0.472319  0.439683  0.470865  0.449012  0.459043   \n",
       "    52.0         0.289672  0.333714  0.288954  0.379036  0.464840  0.503392   \n",
       "    53.0         0.408658  0.363922  0.384089  0.383768  0.495251  0.534231   \n",
       "    54.0         0.394507  0.415821  0.365229  0.413066  0.515140  0.556148   \n",
       "    55.0         0.372250  0.471655  0.382377  0.485190  0.409469  0.497607   \n",
       "    56.0         0.450867  0.533203  0.460649  0.528068  0.432870  0.500047   \n",
       "    57.0         0.522490  0.467214  0.489243  0.486342  0.469642  0.534365   \n",
       "    58.0         0.428265  0.481899  0.360731  0.455341  0.464040  0.470022   \n",
       "    59.0         0.425697  0.443003  0.345634  0.338100  0.487004  0.475806   \n",
       "    60.0         0.428545  0.485560  0.367866  0.438706  0.508617  0.521912   \n",
       "    61.0         0.443112  0.537733  0.397926  0.535974  0.469231  0.523646   \n",
       "    62.0         0.411753  0.480334  0.396796  0.478317  0.478693  0.550527   \n",
       "    63.0         0.392741  0.438965  0.387190  0.434401  0.404633  0.477792   \n",
       "    64.0         0.462021  0.484946  0.423946  0.480213  0.458718  0.485950   \n",
       "    65.0         0.384998  0.484273  0.376069  0.473330  0.483462  0.505638   \n",
       "    66.0         0.472467  0.515086  0.398503  0.511535  0.401946  0.459444   \n",
       "    67.0         0.444525  0.488841  0.414096  0.472164  0.471776  0.512027   \n",
       "    68.0         0.386178  0.370519  0.401951  0.364592  0.499287  0.488592   \n",
       "    69.0         0.444872  0.461017  0.336292  0.409555  0.477272  0.535823   \n",
       "    70.0         0.406763  0.420498  0.414044  0.420534  0.394608  0.471671   \n",
       "    71.0         0.415728  0.487428  0.425795  0.486679  0.452769  0.472229   \n",
       "    72.0         0.442808  0.504454  0.431128  0.496055  0.434589  0.512512   \n",
       "    73.0         0.451600  0.531204  0.459936  0.543633  0.447364  0.543243   \n",
       "    74.0         0.418354  0.444321  0.420425  0.445109  0.456053  0.473899   \n",
       "    75.0         0.324110  0.424198  0.236731  0.407496  0.343313  0.342349   \n",
       "    76.0         0.382361  0.445684  0.361471  0.430837  0.441623  0.473105   \n",
       "    77.0         0.445548  0.509010  0.324138  0.421720  0.517330  0.557018   \n",
       "    78.0         0.437315  0.480847  0.329985  0.377106  0.433371  0.450538   \n",
       "    79.0         0.449707  0.488906  0.441434  0.487836  0.376769  0.424680   \n",
       "    80.0         0.430740  0.533332  0.459228  0.537714  0.427250  0.445980   \n",
       "    81.0         0.405685  0.380666  0.405428  0.410265  0.468343  0.512032   \n",
       "    82.0         0.282880  0.411413  0.211684  0.345705  0.456484  0.507412   \n",
       "    83.0         0.436591  0.503496  0.429974  0.500843  0.461493  0.527037   \n",
       "    84.0         0.418366  0.419744  0.412889  0.399055  0.511221  0.549968   \n",
       "    85.0         0.432117  0.451533  0.410348  0.378872  0.450651  0.446998   \n",
       "    86.0         0.476828  0.468607  0.414228  0.399621  0.464082  0.477902   \n",
       "    87.0         0.524748  0.538720  0.471599  0.526151  0.450251  0.465265   \n",
       "    88.0         0.449297  0.543471  0.365648  0.504055  0.486587  0.542846   \n",
       "    89.0         0.459170  0.534882  0.471245  0.547434  0.529792  0.562644   \n",
       "    90.0         0.503354  0.524939  0.496997  0.537740  0.453807  0.458390   \n",
       "    91.0         0.441129  0.520006  0.424023  0.538422  0.509335  0.575805   \n",
       "\n",
       "dataset                                                ... segmentsemantic  \\\n",
       "train_size                                             ...            1024   \n",
       "features            POF      POFE       PSF      PSFE  ...            PSOF   \n",
       "    data_seed                                          ...                   \n",
       "tau 42.0       0.414067  0.393243  0.446897  0.435244  ...        0.812955   \n",
       "    43.0       0.527333  0.528291  0.543472  0.547068  ...        0.810909   \n",
       "    44.0       0.437029  0.459634  0.538023  0.533351  ...        0.808056   \n",
       "    45.0       0.475479  0.498735  0.551270  0.539651  ...        0.798316   \n",
       "    46.0       0.512148  0.546277  0.565986  0.567577  ...        0.803855   \n",
       "    47.0       0.503733  0.498241  0.532132  0.534352  ...        0.818755   \n",
       "    48.0       0.519222  0.498947  0.590629  0.577101  ...        0.812630   \n",
       "    49.0       0.489264  0.498192  0.557595  0.535445  ...        0.809851   \n",
       "    50.0       0.480001  0.463224  0.583988  0.591878  ...        0.802293   \n",
       "    51.0       0.456846  0.463156  0.553558  0.554331  ...        0.815811   \n",
       "    52.0       0.451444  0.492122  0.553906  0.563883  ...        0.788301   \n",
       "    53.0       0.508030  0.495445  0.583055  0.582124  ...        0.802723   \n",
       "    54.0       0.542184  0.555053  0.597861  0.595361  ...        0.805851   \n",
       "    55.0       0.450997  0.493886  0.549892  0.547261  ...        0.807429   \n",
       "    56.0       0.524650  0.511771  0.562862  0.570370  ...        0.815057   \n",
       "    57.0       0.531954  0.539248  0.543344  0.532582  ...        0.804742   \n",
       "    58.0       0.471191  0.473354  0.571521  0.576930  ...        0.798551   \n",
       "    59.0       0.484121  0.471307  0.559728  0.560367  ...        0.817548   \n",
       "    60.0       0.496805  0.530486  0.613104  0.612964  ...        0.808595   \n",
       "    61.0       0.492391  0.533480  0.562994  0.568278  ...        0.809545   \n",
       "    62.0       0.507676  0.548775  0.571787  0.569913  ...        0.813050   \n",
       "    63.0       0.480202  0.476970  0.518887  0.506672  ...        0.802310   \n",
       "    64.0       0.456959  0.474778  0.555447  0.552363  ...        0.811390   \n",
       "    65.0       0.504413  0.507473  0.560316  0.571772  ...        0.820872   \n",
       "    66.0       0.446898  0.459628  0.524026  0.547135  ...        0.816906   \n",
       "    67.0       0.501978  0.516334  0.520650  0.542353  ...        0.802359   \n",
       "    68.0       0.492998  0.480144  0.521859  0.527602  ...        0.798427   \n",
       "    69.0       0.507645  0.523621  0.568651  0.566717  ...        0.807717   \n",
       "    70.0       0.450742  0.456360  0.557655  0.558694  ...        0.812347   \n",
       "    71.0       0.456261  0.461682  0.577379  0.571362  ...        0.805774   \n",
       "    72.0       0.496525  0.519361  0.566281  0.556255  ...        0.809388   \n",
       "    73.0       0.503990  0.542657  0.564631  0.572310  ...        0.820304   \n",
       "    74.0       0.460151  0.467178  0.561458  0.558004  ...        0.804328   \n",
       "    75.0       0.324510  0.332467  0.549908  0.523725  ...        0.805837   \n",
       "    76.0       0.461089  0.462171  0.536352  0.528878  ...        0.803606   \n",
       "    77.0       0.540003  0.559816  0.603023  0.606938  ...        0.808206   \n",
       "    78.0       0.455683  0.464082  0.546695  0.544781  ...        0.808698   \n",
       "    79.0       0.420066  0.416980  0.437145  0.457966  ...        0.808404   \n",
       "    80.0       0.425059  0.436164  0.506193  0.513135  ...        0.809383   \n",
       "    81.0       0.486535  0.488639  0.557980  0.553580  ...        0.810023   \n",
       "    82.0       0.475760  0.499485  0.558372  0.558238  ...        0.811192   \n",
       "    83.0       0.510162  0.532898  0.576101  0.573442  ...        0.809842   \n",
       "    84.0       0.551914  0.550263  0.606836  0.603732  ...        0.803018   \n",
       "    85.0       0.453668  0.462816  0.505245  0.501486  ...        0.800670   \n",
       "    86.0       0.480698  0.474828  0.547670  0.542176  ...        0.821147   \n",
       "    87.0       0.461750  0.470355  0.542003  0.545689  ...        0.811290   \n",
       "    88.0       0.497457  0.543542  0.565296  0.562801  ...        0.799745   \n",
       "    89.0       0.564687  0.562938  0.583568  0.576327  ...        0.802271   \n",
       "    90.0       0.462958  0.456944  0.525057  0.523074  ...        0.818098   \n",
       "    91.0       0.573798  0.577690  0.588576  0.569913  ...        0.806699   \n",
       "\n",
       "dataset                                                                    \\\n",
       "train_size                                                                  \n",
       "features          PSOFE         S        SE        SF       SFE        SO   \n",
       "    data_seed                                                               \n",
       "tau 42.0       0.811572  0.815755  0.818922  0.814376  0.815722  0.823819   \n",
       "    43.0       0.811396  0.808229  0.814645  0.806100  0.809826  0.819391   \n",
       "    44.0       0.810687  0.814685  0.817459  0.808838  0.814163  0.819446   \n",
       "    45.0       0.800458  0.806476  0.813537  0.809206  0.814211  0.815500   \n",
       "    46.0       0.805209  0.803109  0.810104  0.799791  0.803901  0.809391   \n",
       "    47.0       0.820654  0.818358  0.821000  0.819459  0.823241  0.825460   \n",
       "    48.0       0.813743  0.814424  0.819603  0.814962  0.820702  0.820011   \n",
       "    49.0       0.811654  0.806307  0.810321  0.807013  0.809316  0.816454   \n",
       "    50.0       0.803525  0.800233  0.803732  0.804336  0.805822  0.811242   \n",
       "    51.0       0.817167  0.814098  0.816830  0.815345  0.818696  0.827233   \n",
       "    52.0       0.790610  0.798475  0.796740  0.798521  0.797968  0.804283   \n",
       "    53.0       0.803777  0.807100  0.811141  0.805158  0.808840  0.816396   \n",
       "    54.0       0.808361  0.811195  0.816502  0.807394  0.812652  0.817442   \n",
       "    55.0       0.808605  0.809048  0.814411  0.807321  0.814029  0.817386   \n",
       "    56.0       0.817170  0.811867  0.815758  0.813700  0.818568  0.823398   \n",
       "    57.0       0.806516  0.806061  0.809158  0.805732  0.810079  0.810383   \n",
       "    58.0       0.799235  0.812031  0.814649  0.810027  0.812996  0.818680   \n",
       "    59.0       0.818681  0.822502  0.828222  0.823398  0.827337  0.828428   \n",
       "    60.0       0.811187  0.811194  0.817273  0.811039  0.815452  0.814387   \n",
       "    61.0       0.810439  0.812523  0.813721  0.810177  0.814102  0.818825   \n",
       "    62.0       0.813464  0.804087  0.804914  0.802510  0.805516  0.811581   \n",
       "    63.0       0.805129  0.804961  0.806758  0.806755  0.810059  0.814292   \n",
       "    64.0       0.813269  0.812237  0.815649  0.812730  0.815218  0.821925   \n",
       "    65.0       0.819922  0.823317  0.824160  0.822256  0.824262  0.830681   \n",
       "    66.0       0.817409  0.818252  0.822902  0.816464  0.820933  0.823339   \n",
       "    67.0       0.804147  0.810974  0.813978  0.813238  0.814522  0.819978   \n",
       "    68.0       0.800845  0.801646  0.803236  0.799208  0.801154  0.808956   \n",
       "    69.0       0.808059  0.816316  0.820005  0.816136  0.819755  0.822270   \n",
       "    70.0       0.814946  0.813572  0.816447  0.812567  0.816293  0.820514   \n",
       "    71.0       0.807559  0.815014  0.816265  0.811232  0.813476  0.820677   \n",
       "    72.0       0.810294  0.816764  0.820932  0.811212  0.816189  0.824368   \n",
       "    73.0       0.820210  0.814263  0.818977  0.815084  0.821279  0.824571   \n",
       "    74.0       0.805103  0.807979  0.814011  0.810260  0.818037  0.818267   \n",
       "    75.0       0.807930  0.807651  0.813818  0.808418  0.812921  0.815190   \n",
       "    76.0       0.805183  0.810403  0.813860  0.809542  0.815695  0.818077   \n",
       "    77.0       0.809694  0.812692  0.815559  0.811178  0.813078  0.823485   \n",
       "    78.0       0.807394  0.807814  0.810363  0.809338  0.810898  0.819703   \n",
       "    79.0       0.810063  0.812399  0.815306  0.810586  0.814999  0.818756   \n",
       "    80.0       0.809929  0.804570  0.812018  0.809386  0.815653  0.812754   \n",
       "    81.0       0.811288  0.816905  0.819933  0.817992  0.821177  0.822001   \n",
       "    82.0       0.811876  0.815031  0.820102  0.813224  0.820246  0.822092   \n",
       "    83.0       0.811600  0.811533  0.816673  0.811024  0.816309  0.819257   \n",
       "    84.0       0.805110  0.804852  0.807378  0.804385  0.805955  0.808862   \n",
       "    85.0       0.802204  0.802057  0.807284  0.804990  0.806574  0.809116   \n",
       "    86.0       0.819938  0.815950  0.820419  0.817476  0.820345  0.825251   \n",
       "    87.0       0.812745  0.810451  0.812095  0.809605  0.813258  0.815255   \n",
       "    88.0       0.801181  0.801443  0.802680  0.797800  0.798715  0.812282   \n",
       "    89.0       0.801444  0.802982  0.806235  0.806868  0.809044  0.811141   \n",
       "    90.0       0.818246  0.814838  0.816703  0.817758  0.819857  0.821587   \n",
       "    91.0       0.809392  0.810504  0.816423  0.811094  0.815139  0.818213   \n",
       "\n",
       "dataset                                      \n",
       "train_size                                   \n",
       "features            SOE       SOF      SOFE  \n",
       "    data_seed                                \n",
       "tau 42.0       0.823504  0.823493  0.824787  \n",
       "    43.0       0.821001  0.817118  0.818170  \n",
       "    44.0       0.821568  0.817803  0.820987  \n",
       "    45.0       0.817636  0.816148  0.817711  \n",
       "    46.0       0.812305  0.809575  0.810918  \n",
       "    47.0       0.825105  0.828374  0.827191  \n",
       "    48.0       0.822850  0.823947  0.826776  \n",
       "    49.0       0.816214  0.816208  0.816083  \n",
       "    50.0       0.810887  0.813522  0.813493  \n",
       "    51.0       0.827269  0.824106  0.824842  \n",
       "    52.0       0.800831  0.803252  0.802319  \n",
       "    53.0       0.817010  0.815505  0.817706  \n",
       "    54.0       0.820401  0.816907  0.819137  \n",
       "    55.0       0.817833  0.817889  0.818641  \n",
       "    56.0       0.824427  0.825504  0.826927  \n",
       "    57.0       0.810715  0.812978  0.814246  \n",
       "    58.0       0.819159  0.817263  0.818387  \n",
       "    59.0       0.830034  0.828108  0.830616  \n",
       "    60.0       0.816131  0.814708  0.815287  \n",
       "    61.0       0.819046  0.818950  0.818900  \n",
       "    62.0       0.810964  0.813052  0.812833  \n",
       "    63.0       0.816105  0.817146  0.817508  \n",
       "    64.0       0.823148  0.822763  0.823785  \n",
       "    65.0       0.829854  0.830658  0.830044  \n",
       "    66.0       0.825014  0.821768  0.824262  \n",
       "    67.0       0.821315  0.819505  0.820510  \n",
       "    68.0       0.806913  0.807767  0.805750  \n",
       "    69.0       0.823716  0.823292  0.825138  \n",
       "    70.0       0.822170  0.822620  0.824377  \n",
       "    71.0       0.820811  0.817943  0.818614  \n",
       "    72.0       0.826224  0.819033  0.820626  \n",
       "    73.0       0.826311  0.828499  0.829574  \n",
       "    74.0       0.820209  0.815793  0.818536  \n",
       "    75.0       0.817270  0.815524  0.818626  \n",
       "    76.0       0.818412  0.820285  0.820777  \n",
       "    77.0       0.824389  0.822401  0.823679  \n",
       "    78.0       0.819592  0.819999  0.821310  \n",
       "    79.0       0.818739  0.818378  0.820412  \n",
       "    80.0       0.815720  0.817320  0.819245  \n",
       "    81.0       0.822963  0.824974  0.826532  \n",
       "    82.0       0.822546  0.821560  0.824970  \n",
       "    83.0       0.819629  0.820161  0.819977  \n",
       "    84.0       0.808655  0.810547  0.810691  \n",
       "    85.0       0.810020  0.809635  0.811281  \n",
       "    86.0       0.826909  0.827867  0.827881  \n",
       "    87.0       0.816461  0.817617  0.818534  \n",
       "    88.0       0.811795  0.808997  0.809943  \n",
       "    89.0       0.810632  0.814105  0.814768  \n",
       "    90.0       0.824053  0.824475  0.826558  \n",
       "    91.0       0.819665  0.821018  0.820283  \n",
       "\n",
       "[50 rows x 420 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data = data[['dataset', 'train_size', 'features', 'data_seed', 'tau']]\n",
    "all_data = all_data.set_index(['dataset', 'train_size', 'features', 'data_seed'])\n",
    "all_data = all_data.unstack(3).T\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Onehot</th>\n",
       "      <th>GRAF</th>\n",
       "      <th>Onehot + F&amp;P</th>\n",
       "      <th>GRAF + Onehot</th>\n",
       "      <th>GRAF + F&amp;P</th>\n",
       "      <th>GRAF + Onehot + F&amp;P</th>\n",
       "      <th>ZCP + F&amp;P</th>\n",
       "      <th>ZCP + Onehot + F&amp;P</th>\n",
       "      <th>ZCP + GRAF + F&amp;P</th>\n",
       "      <th>Everything</th>\n",
       "      <th>Onehot + PE</th>\n",
       "      <th>GRAF + PE</th>\n",
       "      <th>Onehot + F&amp;P + PE</th>\n",
       "      <th>GRAF + Onehot + PE</th>\n",
       "      <th>GRAF + F&amp;P + PE</th>\n",
       "      <th>GRAF + Onehot + F&amp;P + PE</th>\n",
       "      <th>ZCP + F&amp;P + PE</th>\n",
       "      <th>ZCP + Onehot + F&amp;P + PE</th>\n",
       "      <th>ZCP + GRAF + F&amp;P + PE</th>\n",
       "      <th>Everything + PE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">autoencoder</th>\n",
       "      <th>32</th>\n",
       "      <td>0.421313</td>\n",
       "      <td>0.556843</td>\n",
       "      <td>0.394335</td>\n",
       "      <td>0.553655</td>\n",
       "      <td>0.557512</td>\n",
       "      <td>0.553251</td>\n",
       "      <td>0.459022</td>\n",
       "      <td>0.483622</td>\n",
       "      <td>0.553337</td>\n",
       "      <td>0.552670</td>\n",
       "      <td>0.465723</td>\n",
       "      <td>0.553321</td>\n",
       "      <td>0.453614</td>\n",
       "      <td>0.551072</td>\n",
       "      <td>0.552338</td>\n",
       "      <td>0.551551</td>\n",
       "      <td>0.496911</td>\n",
       "      <td>0.493423</td>\n",
       "      <td>0.552263</td>\n",
       "      <td>0.549546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.548206</td>\n",
       "      <td>0.594436</td>\n",
       "      <td>0.542333</td>\n",
       "      <td>0.593622</td>\n",
       "      <td>0.594830</td>\n",
       "      <td>0.593993</td>\n",
       "      <td>0.519493</td>\n",
       "      <td>0.554785</td>\n",
       "      <td>0.595638</td>\n",
       "      <td>0.595692</td>\n",
       "      <td>0.543733</td>\n",
       "      <td>0.589315</td>\n",
       "      <td>0.555802</td>\n",
       "      <td>0.588122</td>\n",
       "      <td>0.590485</td>\n",
       "      <td>0.589116</td>\n",
       "      <td>0.568195</td>\n",
       "      <td>0.568264</td>\n",
       "      <td>0.593529</td>\n",
       "      <td>0.593486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.603622</td>\n",
       "      <td>0.626714</td>\n",
       "      <td>0.603980</td>\n",
       "      <td>0.627096</td>\n",
       "      <td>0.626479</td>\n",
       "      <td>0.626283</td>\n",
       "      <td>0.558745</td>\n",
       "      <td>0.598803</td>\n",
       "      <td>0.634199</td>\n",
       "      <td>0.634043</td>\n",
       "      <td>0.598362</td>\n",
       "      <td>0.624169</td>\n",
       "      <td>0.607746</td>\n",
       "      <td>0.623032</td>\n",
       "      <td>0.625450</td>\n",
       "      <td>0.624008</td>\n",
       "      <td>0.612310</td>\n",
       "      <td>0.612432</td>\n",
       "      <td>0.633022</td>\n",
       "      <td>0.632321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">class_object</th>\n",
       "      <th>32</th>\n",
       "      <td>0.234895</td>\n",
       "      <td>0.532616</td>\n",
       "      <td>0.458404</td>\n",
       "      <td>0.529090</td>\n",
       "      <td>0.530995</td>\n",
       "      <td>0.524659</td>\n",
       "      <td>0.408889</td>\n",
       "      <td>0.433837</td>\n",
       "      <td>0.503990</td>\n",
       "      <td>0.506323</td>\n",
       "      <td>0.225390</td>\n",
       "      <td>0.528369</td>\n",
       "      <td>0.448298</td>\n",
       "      <td>0.524140</td>\n",
       "      <td>0.527001</td>\n",
       "      <td>0.525740</td>\n",
       "      <td>0.433785</td>\n",
       "      <td>0.431959</td>\n",
       "      <td>0.502536</td>\n",
       "      <td>0.503252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.407574</td>\n",
       "      <td>0.602710</td>\n",
       "      <td>0.548253</td>\n",
       "      <td>0.600464</td>\n",
       "      <td>0.601996</td>\n",
       "      <td>0.599850</td>\n",
       "      <td>0.526179</td>\n",
       "      <td>0.551099</td>\n",
       "      <td>0.597041</td>\n",
       "      <td>0.596796</td>\n",
       "      <td>0.348737</td>\n",
       "      <td>0.603204</td>\n",
       "      <td>0.549574</td>\n",
       "      <td>0.600406</td>\n",
       "      <td>0.601715</td>\n",
       "      <td>0.599485</td>\n",
       "      <td>0.550789</td>\n",
       "      <td>0.554665</td>\n",
       "      <td>0.598251</td>\n",
       "      <td>0.596890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.614788</td>\n",
       "      <td>0.635613</td>\n",
       "      <td>0.621062</td>\n",
       "      <td>0.637634</td>\n",
       "      <td>0.635773</td>\n",
       "      <td>0.637896</td>\n",
       "      <td>0.595026</td>\n",
       "      <td>0.621737</td>\n",
       "      <td>0.646673</td>\n",
       "      <td>0.647139</td>\n",
       "      <td>0.597773</td>\n",
       "      <td>0.638217</td>\n",
       "      <td>0.623236</td>\n",
       "      <td>0.638727</td>\n",
       "      <td>0.638075</td>\n",
       "      <td>0.638813</td>\n",
       "      <td>0.622268</td>\n",
       "      <td>0.627647</td>\n",
       "      <td>0.647616</td>\n",
       "      <td>0.647784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">class_scene</th>\n",
       "      <th>32</th>\n",
       "      <td>0.288730</td>\n",
       "      <td>0.578467</td>\n",
       "      <td>0.511530</td>\n",
       "      <td>0.575408</td>\n",
       "      <td>0.579700</td>\n",
       "      <td>0.573526</td>\n",
       "      <td>0.517780</td>\n",
       "      <td>0.532806</td>\n",
       "      <td>0.584517</td>\n",
       "      <td>0.585900</td>\n",
       "      <td>0.304356</td>\n",
       "      <td>0.574742</td>\n",
       "      <td>0.505060</td>\n",
       "      <td>0.568462</td>\n",
       "      <td>0.578185</td>\n",
       "      <td>0.572787</td>\n",
       "      <td>0.530234</td>\n",
       "      <td>0.533433</td>\n",
       "      <td>0.580762</td>\n",
       "      <td>0.583402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.508904</td>\n",
       "      <td>0.673935</td>\n",
       "      <td>0.609074</td>\n",
       "      <td>0.671926</td>\n",
       "      <td>0.671787</td>\n",
       "      <td>0.672135</td>\n",
       "      <td>0.601883</td>\n",
       "      <td>0.635012</td>\n",
       "      <td>0.686309</td>\n",
       "      <td>0.686592</td>\n",
       "      <td>0.454942</td>\n",
       "      <td>0.670808</td>\n",
       "      <td>0.604639</td>\n",
       "      <td>0.669925</td>\n",
       "      <td>0.670273</td>\n",
       "      <td>0.670729</td>\n",
       "      <td>0.625201</td>\n",
       "      <td>0.639498</td>\n",
       "      <td>0.685535</td>\n",
       "      <td>0.685654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.721328</td>\n",
       "      <td>0.747091</td>\n",
       "      <td>0.721052</td>\n",
       "      <td>0.752149</td>\n",
       "      <td>0.746838</td>\n",
       "      <td>0.753042</td>\n",
       "      <td>0.714082</td>\n",
       "      <td>0.739460</td>\n",
       "      <td>0.766442</td>\n",
       "      <td>0.767388</td>\n",
       "      <td>0.716411</td>\n",
       "      <td>0.752225</td>\n",
       "      <td>0.726636</td>\n",
       "      <td>0.754096</td>\n",
       "      <td>0.752510</td>\n",
       "      <td>0.755378</td>\n",
       "      <td>0.729808</td>\n",
       "      <td>0.741789</td>\n",
       "      <td>0.766921</td>\n",
       "      <td>0.767658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">jigsaw</th>\n",
       "      <th>32</th>\n",
       "      <td>0.220700</td>\n",
       "      <td>0.468788</td>\n",
       "      <td>0.413653</td>\n",
       "      <td>0.468815</td>\n",
       "      <td>0.468426</td>\n",
       "      <td>0.467235</td>\n",
       "      <td>0.383736</td>\n",
       "      <td>0.397433</td>\n",
       "      <td>0.454916</td>\n",
       "      <td>0.457085</td>\n",
       "      <td>0.196527</td>\n",
       "      <td>0.462256</td>\n",
       "      <td>0.379203</td>\n",
       "      <td>0.459374</td>\n",
       "      <td>0.460878</td>\n",
       "      <td>0.459551</td>\n",
       "      <td>0.390928</td>\n",
       "      <td>0.392695</td>\n",
       "      <td>0.448945</td>\n",
       "      <td>0.451484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.351782</td>\n",
       "      <td>0.534266</td>\n",
       "      <td>0.489417</td>\n",
       "      <td>0.534530</td>\n",
       "      <td>0.534987</td>\n",
       "      <td>0.535054</td>\n",
       "      <td>0.463346</td>\n",
       "      <td>0.479235</td>\n",
       "      <td>0.522844</td>\n",
       "      <td>0.522922</td>\n",
       "      <td>0.307971</td>\n",
       "      <td>0.523641</td>\n",
       "      <td>0.461666</td>\n",
       "      <td>0.523187</td>\n",
       "      <td>0.525021</td>\n",
       "      <td>0.524467</td>\n",
       "      <td>0.469321</td>\n",
       "      <td>0.473196</td>\n",
       "      <td>0.519360</td>\n",
       "      <td>0.518765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.526285</td>\n",
       "      <td>0.572890</td>\n",
       "      <td>0.555687</td>\n",
       "      <td>0.577282</td>\n",
       "      <td>0.574025</td>\n",
       "      <td>0.578783</td>\n",
       "      <td>0.541875</td>\n",
       "      <td>0.559278</td>\n",
       "      <td>0.590881</td>\n",
       "      <td>0.591596</td>\n",
       "      <td>0.487619</td>\n",
       "      <td>0.575126</td>\n",
       "      <td>0.548040</td>\n",
       "      <td>0.576778</td>\n",
       "      <td>0.577161</td>\n",
       "      <td>0.578921</td>\n",
       "      <td>0.554188</td>\n",
       "      <td>0.560315</td>\n",
       "      <td>0.590162</td>\n",
       "      <td>0.590682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">normal</th>\n",
       "      <th>32</th>\n",
       "      <td>0.312486</td>\n",
       "      <td>0.497744</td>\n",
       "      <td>0.464070</td>\n",
       "      <td>0.492920</td>\n",
       "      <td>0.498707</td>\n",
       "      <td>0.496277</td>\n",
       "      <td>0.478363</td>\n",
       "      <td>0.481073</td>\n",
       "      <td>0.511155</td>\n",
       "      <td>0.508163</td>\n",
       "      <td>0.312120</td>\n",
       "      <td>0.498806</td>\n",
       "      <td>0.468068</td>\n",
       "      <td>0.497085</td>\n",
       "      <td>0.500688</td>\n",
       "      <td>0.499470</td>\n",
       "      <td>0.483857</td>\n",
       "      <td>0.484608</td>\n",
       "      <td>0.511020</td>\n",
       "      <td>0.509924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.478225</td>\n",
       "      <td>0.544988</td>\n",
       "      <td>0.525458</td>\n",
       "      <td>0.547507</td>\n",
       "      <td>0.546959</td>\n",
       "      <td>0.550082</td>\n",
       "      <td>0.525163</td>\n",
       "      <td>0.537861</td>\n",
       "      <td>0.552622</td>\n",
       "      <td>0.554792</td>\n",
       "      <td>0.440136</td>\n",
       "      <td>0.544974</td>\n",
       "      <td>0.523986</td>\n",
       "      <td>0.545612</td>\n",
       "      <td>0.548585</td>\n",
       "      <td>0.550429</td>\n",
       "      <td>0.534639</td>\n",
       "      <td>0.538966</td>\n",
       "      <td>0.552854</td>\n",
       "      <td>0.554677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.551938</td>\n",
       "      <td>0.599649</td>\n",
       "      <td>0.595424</td>\n",
       "      <td>0.601121</td>\n",
       "      <td>0.609254</td>\n",
       "      <td>0.612095</td>\n",
       "      <td>0.587558</td>\n",
       "      <td>0.604638</td>\n",
       "      <td>0.617396</td>\n",
       "      <td>0.619998</td>\n",
       "      <td>0.549827</td>\n",
       "      <td>0.599521</td>\n",
       "      <td>0.590677</td>\n",
       "      <td>0.600025</td>\n",
       "      <td>0.610960</td>\n",
       "      <td>0.612142</td>\n",
       "      <td>0.597961</td>\n",
       "      <td>0.605123</td>\n",
       "      <td>0.618073</td>\n",
       "      <td>0.619527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">room_layout</th>\n",
       "      <th>32</th>\n",
       "      <td>0.200374</td>\n",
       "      <td>0.670581</td>\n",
       "      <td>0.585850</td>\n",
       "      <td>0.662958</td>\n",
       "      <td>0.670570</td>\n",
       "      <td>0.665898</td>\n",
       "      <td>0.608743</td>\n",
       "      <td>0.612841</td>\n",
       "      <td>0.662341</td>\n",
       "      <td>0.667496</td>\n",
       "      <td>0.228915</td>\n",
       "      <td>0.664561</td>\n",
       "      <td>0.596388</td>\n",
       "      <td>0.662800</td>\n",
       "      <td>0.669626</td>\n",
       "      <td>0.671340</td>\n",
       "      <td>0.619003</td>\n",
       "      <td>0.608976</td>\n",
       "      <td>0.659266</td>\n",
       "      <td>0.661733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.363449</td>\n",
       "      <td>0.736817</td>\n",
       "      <td>0.679315</td>\n",
       "      <td>0.735959</td>\n",
       "      <td>0.738930</td>\n",
       "      <td>0.738006</td>\n",
       "      <td>0.717225</td>\n",
       "      <td>0.722179</td>\n",
       "      <td>0.746044</td>\n",
       "      <td>0.746088</td>\n",
       "      <td>0.353174</td>\n",
       "      <td>0.735973</td>\n",
       "      <td>0.698139</td>\n",
       "      <td>0.735317</td>\n",
       "      <td>0.738633</td>\n",
       "      <td>0.737586</td>\n",
       "      <td>0.721467</td>\n",
       "      <td>0.722429</td>\n",
       "      <td>0.745463</td>\n",
       "      <td>0.745486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.666984</td>\n",
       "      <td>0.783519</td>\n",
       "      <td>0.770297</td>\n",
       "      <td>0.785014</td>\n",
       "      <td>0.785984</td>\n",
       "      <td>0.787715</td>\n",
       "      <td>0.775395</td>\n",
       "      <td>0.781966</td>\n",
       "      <td>0.794845</td>\n",
       "      <td>0.795129</td>\n",
       "      <td>0.645333</td>\n",
       "      <td>0.785569</td>\n",
       "      <td>0.775027</td>\n",
       "      <td>0.785489</td>\n",
       "      <td>0.788125</td>\n",
       "      <td>0.788123</td>\n",
       "      <td>0.781485</td>\n",
       "      <td>0.782810</td>\n",
       "      <td>0.794489</td>\n",
       "      <td>0.794838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">segmentsemantic</th>\n",
       "      <th>32</th>\n",
       "      <td>0.419280</td>\n",
       "      <td>0.680432</td>\n",
       "      <td>0.624736</td>\n",
       "      <td>0.682903</td>\n",
       "      <td>0.679803</td>\n",
       "      <td>0.684085</td>\n",
       "      <td>0.622247</td>\n",
       "      <td>0.641784</td>\n",
       "      <td>0.681632</td>\n",
       "      <td>0.684334</td>\n",
       "      <td>0.453276</td>\n",
       "      <td>0.678739</td>\n",
       "      <td>0.630471</td>\n",
       "      <td>0.681005</td>\n",
       "      <td>0.679907</td>\n",
       "      <td>0.682456</td>\n",
       "      <td>0.629278</td>\n",
       "      <td>0.643987</td>\n",
       "      <td>0.680126</td>\n",
       "      <td>0.683980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>0.640405</td>\n",
       "      <td>0.749969</td>\n",
       "      <td>0.693921</td>\n",
       "      <td>0.753892</td>\n",
       "      <td>0.744508</td>\n",
       "      <td>0.750335</td>\n",
       "      <td>0.682198</td>\n",
       "      <td>0.710087</td>\n",
       "      <td>0.742655</td>\n",
       "      <td>0.747360</td>\n",
       "      <td>0.605652</td>\n",
       "      <td>0.750394</td>\n",
       "      <td>0.701436</td>\n",
       "      <td>0.753751</td>\n",
       "      <td>0.745075</td>\n",
       "      <td>0.749920</td>\n",
       "      <td>0.694436</td>\n",
       "      <td>0.712482</td>\n",
       "      <td>0.743496</td>\n",
       "      <td>0.747067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>0.790092</td>\n",
       "      <td>0.810502</td>\n",
       "      <td>0.781490</td>\n",
       "      <td>0.818240</td>\n",
       "      <td>0.810244</td>\n",
       "      <td>0.818721</td>\n",
       "      <td>0.736128</td>\n",
       "      <td>0.770948</td>\n",
       "      <td>0.800985</td>\n",
       "      <td>0.808301</td>\n",
       "      <td>0.794291</td>\n",
       "      <td>0.814176</td>\n",
       "      <td>0.794182</td>\n",
       "      <td>0.819083</td>\n",
       "      <td>0.813884</td>\n",
       "      <td>0.819803</td>\n",
       "      <td>0.753820</td>\n",
       "      <td>0.775151</td>\n",
       "      <td>0.803409</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Onehot      GRAF  Onehot + F&P  GRAF + Onehot  \\\n",
       "dataset         train_size                                                    \n",
       "autoencoder     32          0.421313  0.556843      0.394335       0.553655   \n",
       "                128         0.548206  0.594436      0.542333       0.593622   \n",
       "                1024        0.603622  0.626714      0.603980       0.627096   \n",
       "class_object    32          0.234895  0.532616      0.458404       0.529090   \n",
       "                128         0.407574  0.602710      0.548253       0.600464   \n",
       "                1024        0.614788  0.635613      0.621062       0.637634   \n",
       "class_scene     32          0.288730  0.578467      0.511530       0.575408   \n",
       "                128         0.508904  0.673935      0.609074       0.671926   \n",
       "                1024        0.721328  0.747091      0.721052       0.752149   \n",
       "jigsaw          32          0.220700  0.468788      0.413653       0.468815   \n",
       "                128         0.351782  0.534266      0.489417       0.534530   \n",
       "                1024        0.526285  0.572890      0.555687       0.577282   \n",
       "normal          32          0.312486  0.497744      0.464070       0.492920   \n",
       "                128         0.478225  0.544988      0.525458       0.547507   \n",
       "                1024        0.551938  0.599649      0.595424       0.601121   \n",
       "room_layout     32          0.200374  0.670581      0.585850       0.662958   \n",
       "                128         0.363449  0.736817      0.679315       0.735959   \n",
       "                1024        0.666984  0.783519      0.770297       0.785014   \n",
       "segmentsemantic 32          0.419280  0.680432      0.624736       0.682903   \n",
       "                128         0.640405  0.749969      0.693921       0.753892   \n",
       "                1024        0.790092  0.810502      0.781490       0.818240   \n",
       "\n",
       "                            GRAF + F&P  GRAF + Onehot + F&P  ZCP + F&P  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32            0.557512             0.553251   0.459022   \n",
       "                128           0.594830             0.593993   0.519493   \n",
       "                1024          0.626479             0.626283   0.558745   \n",
       "class_object    32            0.530995             0.524659   0.408889   \n",
       "                128           0.601996             0.599850   0.526179   \n",
       "                1024          0.635773             0.637896   0.595026   \n",
       "class_scene     32            0.579700             0.573526   0.517780   \n",
       "                128           0.671787             0.672135   0.601883   \n",
       "                1024          0.746838             0.753042   0.714082   \n",
       "jigsaw          32            0.468426             0.467235   0.383736   \n",
       "                128           0.534987             0.535054   0.463346   \n",
       "                1024          0.574025             0.578783   0.541875   \n",
       "normal          32            0.498707             0.496277   0.478363   \n",
       "                128           0.546959             0.550082   0.525163   \n",
       "                1024          0.609254             0.612095   0.587558   \n",
       "room_layout     32            0.670570             0.665898   0.608743   \n",
       "                128           0.738930             0.738006   0.717225   \n",
       "                1024          0.785984             0.787715   0.775395   \n",
       "segmentsemantic 32            0.679803             0.684085   0.622247   \n",
       "                128           0.744508             0.750335   0.682198   \n",
       "                1024          0.810244             0.818721   0.736128   \n",
       "\n",
       "                            ZCP + Onehot + F&P  ZCP + GRAF + F&P  Everything  \\\n",
       "dataset         train_size                                                     \n",
       "autoencoder     32                    0.483622          0.553337    0.552670   \n",
       "                128                   0.554785          0.595638    0.595692   \n",
       "                1024                  0.598803          0.634199    0.634043   \n",
       "class_object    32                    0.433837          0.503990    0.506323   \n",
       "                128                   0.551099          0.597041    0.596796   \n",
       "                1024                  0.621737          0.646673    0.647139   \n",
       "class_scene     32                    0.532806          0.584517    0.585900   \n",
       "                128                   0.635012          0.686309    0.686592   \n",
       "                1024                  0.739460          0.766442    0.767388   \n",
       "jigsaw          32                    0.397433          0.454916    0.457085   \n",
       "                128                   0.479235          0.522844    0.522922   \n",
       "                1024                  0.559278          0.590881    0.591596   \n",
       "normal          32                    0.481073          0.511155    0.508163   \n",
       "                128                   0.537861          0.552622    0.554792   \n",
       "                1024                  0.604638          0.617396    0.619998   \n",
       "room_layout     32                    0.612841          0.662341    0.667496   \n",
       "                128                   0.722179          0.746044    0.746088   \n",
       "                1024                  0.781966          0.794845    0.795129   \n",
       "segmentsemantic 32                    0.641784          0.681632    0.684334   \n",
       "                128                   0.710087          0.742655    0.747360   \n",
       "                1024                  0.770948          0.800985    0.808301   \n",
       "\n",
       "                            Onehot + PE  GRAF + PE  Onehot + F&P + PE  \\\n",
       "dataset         train_size                                              \n",
       "autoencoder     32             0.465723   0.553321           0.453614   \n",
       "                128            0.543733   0.589315           0.555802   \n",
       "                1024           0.598362   0.624169           0.607746   \n",
       "class_object    32             0.225390   0.528369           0.448298   \n",
       "                128            0.348737   0.603204           0.549574   \n",
       "                1024           0.597773   0.638217           0.623236   \n",
       "class_scene     32             0.304356   0.574742           0.505060   \n",
       "                128            0.454942   0.670808           0.604639   \n",
       "                1024           0.716411   0.752225           0.726636   \n",
       "jigsaw          32             0.196527   0.462256           0.379203   \n",
       "                128            0.307971   0.523641           0.461666   \n",
       "                1024           0.487619   0.575126           0.548040   \n",
       "normal          32             0.312120   0.498806           0.468068   \n",
       "                128            0.440136   0.544974           0.523986   \n",
       "                1024           0.549827   0.599521           0.590677   \n",
       "room_layout     32             0.228915   0.664561           0.596388   \n",
       "                128            0.353174   0.735973           0.698139   \n",
       "                1024           0.645333   0.785569           0.775027   \n",
       "segmentsemantic 32             0.453276   0.678739           0.630471   \n",
       "                128            0.605652   0.750394           0.701436   \n",
       "                1024           0.794291   0.814176           0.794182   \n",
       "\n",
       "                            GRAF + Onehot + PE  GRAF + F&P + PE  \\\n",
       "dataset         train_size                                        \n",
       "autoencoder     32                    0.551072         0.552338   \n",
       "                128                   0.588122         0.590485   \n",
       "                1024                  0.623032         0.625450   \n",
       "class_object    32                    0.524140         0.527001   \n",
       "                128                   0.600406         0.601715   \n",
       "                1024                  0.638727         0.638075   \n",
       "class_scene     32                    0.568462         0.578185   \n",
       "                128                   0.669925         0.670273   \n",
       "                1024                  0.754096         0.752510   \n",
       "jigsaw          32                    0.459374         0.460878   \n",
       "                128                   0.523187         0.525021   \n",
       "                1024                  0.576778         0.577161   \n",
       "normal          32                    0.497085         0.500688   \n",
       "                128                   0.545612         0.548585   \n",
       "                1024                  0.600025         0.610960   \n",
       "room_layout     32                    0.662800         0.669626   \n",
       "                128                   0.735317         0.738633   \n",
       "                1024                  0.785489         0.788125   \n",
       "segmentsemantic 32                    0.681005         0.679907   \n",
       "                128                   0.753751         0.745075   \n",
       "                1024                  0.819083         0.813884   \n",
       "\n",
       "                            GRAF + Onehot + F&P + PE  ZCP + F&P + PE  \\\n",
       "dataset         train_size                                             \n",
       "autoencoder     32                          0.551551        0.496911   \n",
       "                128                         0.589116        0.568195   \n",
       "                1024                        0.624008        0.612310   \n",
       "class_object    32                          0.525740        0.433785   \n",
       "                128                         0.599485        0.550789   \n",
       "                1024                        0.638813        0.622268   \n",
       "class_scene     32                          0.572787        0.530234   \n",
       "                128                         0.670729        0.625201   \n",
       "                1024                        0.755378        0.729808   \n",
       "jigsaw          32                          0.459551        0.390928   \n",
       "                128                         0.524467        0.469321   \n",
       "                1024                        0.578921        0.554188   \n",
       "normal          32                          0.499470        0.483857   \n",
       "                128                         0.550429        0.534639   \n",
       "                1024                        0.612142        0.597961   \n",
       "room_layout     32                          0.671340        0.619003   \n",
       "                128                         0.737586        0.721467   \n",
       "                1024                        0.788123        0.781485   \n",
       "segmentsemantic 32                          0.682456        0.629278   \n",
       "                128                         0.749920        0.694436   \n",
       "                1024                        0.819803        0.753820   \n",
       "\n",
       "                            ZCP + Onehot + F&P + PE  ZCP + GRAF + F&P + PE  \\\n",
       "dataset         train_size                                                   \n",
       "autoencoder     32                         0.493423               0.552263   \n",
       "                128                        0.568264               0.593529   \n",
       "                1024                       0.612432               0.633022   \n",
       "class_object    32                         0.431959               0.502536   \n",
       "                128                        0.554665               0.598251   \n",
       "                1024                       0.627647               0.647616   \n",
       "class_scene     32                         0.533433               0.580762   \n",
       "                128                        0.639498               0.685535   \n",
       "                1024                       0.741789               0.766921   \n",
       "jigsaw          32                         0.392695               0.448945   \n",
       "                128                        0.473196               0.519360   \n",
       "                1024                       0.560315               0.590162   \n",
       "normal          32                         0.484608               0.511020   \n",
       "                128                        0.538966               0.552854   \n",
       "                1024                       0.605123               0.618073   \n",
       "room_layout     32                         0.608976               0.659266   \n",
       "                128                        0.722429               0.745463   \n",
       "                1024                       0.782810               0.794489   \n",
       "segmentsemantic 32                         0.643987               0.680126   \n",
       "                128                        0.712482               0.743496   \n",
       "                1024                       0.775151               0.803409   \n",
       "\n",
       "                            Everything + PE  \n",
       "dataset         train_size                   \n",
       "autoencoder     32                 0.549546  \n",
       "                128                0.593486  \n",
       "                1024               0.632321  \n",
       "class_object    32                 0.503252  \n",
       "                128                0.596890  \n",
       "                1024               0.647784  \n",
       "class_scene     32                 0.583402  \n",
       "                128                0.685654  \n",
       "                1024               0.767658  \n",
       "jigsaw          32                 0.451484  \n",
       "                128                0.518765  \n",
       "                1024               0.590682  \n",
       "normal          32                 0.509924  \n",
       "                128                0.554677  \n",
       "                1024               0.619527  \n",
       "room_layout     32                 0.661733  \n",
       "                128                0.745486  \n",
       "                1024               0.794838  \n",
       "segmentsemantic 32                 0.683980  \n",
       "                128                0.747067  \n",
       "                1024               0.809524  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_mean = all_data.mean().unstack()\n",
    "table_std = all_data.std().unstack()\n",
    "\n",
    "cols = [c for c in labels if c in table_mean.columns]\n",
    "\n",
    "table_mean = table_mean[cols]\n",
    "table_std = table_std[cols]\n",
    "cols = [features_str[f] for f in cols]\n",
    "table_mean.columns = cols\n",
    "table_std.columns = cols\n",
    "\n",
    "table_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Onehot</th>\n",
       "      <th>GRAF</th>\n",
       "      <th>Onehot + F&amp;P</th>\n",
       "      <th>GRAF + Onehot</th>\n",
       "      <th>GRAF + F&amp;P</th>\n",
       "      <th>GRAF + Onehot + F&amp;P</th>\n",
       "      <th>ZCP + F&amp;P</th>\n",
       "      <th>ZCP + Onehot + F&amp;P</th>\n",
       "      <th>ZCP + GRAF + F&amp;P</th>\n",
       "      <th>Everything</th>\n",
       "      <th>Onehot + PE</th>\n",
       "      <th>GRAF + PE</th>\n",
       "      <th>Onehot + F&amp;P + PE</th>\n",
       "      <th>GRAF + Onehot + PE</th>\n",
       "      <th>GRAF + F&amp;P + PE</th>\n",
       "      <th>GRAF + Onehot + F&amp;P + PE</th>\n",
       "      <th>ZCP + F&amp;P + PE</th>\n",
       "      <th>ZCP + Onehot + F&amp;P + PE</th>\n",
       "      <th>ZCP + GRAF + F&amp;P + PE</th>\n",
       "      <th>Everything + PE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset</th>\n",
       "      <th>train_size</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">autoencoder</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.42131337433001564, 0.05624899815157225)</td>\n",
       "      <td>(0.5568433408681017, 0.030300641200930966)</td>\n",
       "      <td>(0.39433518325506883, 0.06529893856507621)</td>\n",
       "      <td>(0.553655337854133, 0.029414119525018705)</td>\n",
       "      <td>(0.5575116476375594, 0.029004935728709654)</td>\n",
       "      <td>(0.5532510679660605, 0.030044062037441307)</td>\n",
       "      <td>(0.45902171002177083, 0.038039408604563354)</td>\n",
       "      <td>(0.48362247774588213, 0.04267248362902128)</td>\n",
       "      <td>(0.5533373323824581, 0.033489260754819755)</td>\n",
       "      <td>(0.5526703464827988, 0.02955107522111283)</td>\n",
       "      <td>(0.4657229988167805, 0.0610124357603322)</td>\n",
       "      <td>(0.5533214376421388, 0.030054728205799568)</td>\n",
       "      <td>(0.45361390807812696, 0.0671142288987981)</td>\n",
       "      <td>(0.5510717813230754, 0.027895472900277888)</td>\n",
       "      <td>(0.5523382277742221, 0.031192354048323306)</td>\n",
       "      <td>(0.5515506587858673, 0.029107430036449295)</td>\n",
       "      <td>(0.49691059426164785, 0.044916351688701533)</td>\n",
       "      <td>(0.49342264584760254, 0.0460137666200762)</td>\n",
       "      <td>(0.5522630794966668, 0.0327689221462198)</td>\n",
       "      <td>(0.549546420577215, 0.031812394819380654)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.5482058211543945, 0.01870653762409465)</td>\n",
       "      <td>(0.5944363524344953, 0.012474771538213879)</td>\n",
       "      <td>(0.5423329430413283, 0.027434560716485618)</td>\n",
       "      <td>(0.5936217657827685, 0.01284198748412974)</td>\n",
       "      <td>(0.5948304905730829, 0.013222041513842856)</td>\n",
       "      <td>(0.593992878519113, 0.013375181070811046)</td>\n",
       "      <td>(0.5194929287527035, 0.01889628174164421)</td>\n",
       "      <td>(0.5547845812549399, 0.01886557435564282)</td>\n",
       "      <td>(0.595638121785306, 0.01210445357323343)</td>\n",
       "      <td>(0.5956919012198889, 0.01178133291248549)</td>\n",
       "      <td>(0.5437331092948466, 0.015448014912039511)</td>\n",
       "      <td>(0.5893149861686449, 0.013009391508265054)</td>\n",
       "      <td>(0.5558024887152915, 0.017534203532812948)</td>\n",
       "      <td>(0.5881224434580681, 0.013871963829607196)</td>\n",
       "      <td>(0.5904851821368915, 0.013329946933022637)</td>\n",
       "      <td>(0.5891157556368344, 0.013927047940578976)</td>\n",
       "      <td>(0.568195051412126, 0.02138848052055027)</td>\n",
       "      <td>(0.5682644311396872, 0.021306836129126313)</td>\n",
       "      <td>(0.5935291747382364, 0.012355128890622926)</td>\n",
       "      <td>(0.593485619438475, 0.01208570702329643)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.6036222576886772, 0.008387574631852848)</td>\n",
       "      <td>(0.6267138770313302, 0.008365342202978267)</td>\n",
       "      <td>(0.6039797392488457, 0.009442367139994115)</td>\n",
       "      <td>(0.6270962782023536, 0.008405671537546353)</td>\n",
       "      <td>(0.62647923987218, 0.008543311248717395)</td>\n",
       "      <td>(0.626283196877164, 0.008199463900764729)</td>\n",
       "      <td>(0.5587452757035001, 0.009782961913005166)</td>\n",
       "      <td>(0.5988029592890858, 0.00877315854391037)</td>\n",
       "      <td>(0.634198794312162, 0.007672630827658077)</td>\n",
       "      <td>(0.6340426877289199, 0.008239493572385605)</td>\n",
       "      <td>(0.5983618432677335, 0.009192550105327523)</td>\n",
       "      <td>(0.6241689941980385, 0.008486635985324287)</td>\n",
       "      <td>(0.6077455159242197, 0.009736433400369025)</td>\n",
       "      <td>(0.6230323857648686, 0.008465994736139335)</td>\n",
       "      <td>(0.6254497764138837, 0.008771292416416426)</td>\n",
       "      <td>(0.6240078876287803, 0.008650943356647815)</td>\n",
       "      <td>(0.6123098392809944, 0.007557337460666202)</td>\n",
       "      <td>(0.6124321350195674, 0.007327774415383523)</td>\n",
       "      <td>(0.6330223103201728, 0.007976639393444738)</td>\n",
       "      <td>(0.6323210578307649, 0.008076338931314356)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">class_object</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.23489499032400288, 0.05896682705600237)</td>\n",
       "      <td>(0.5326160647975239, 0.0346813651725688)</td>\n",
       "      <td>(0.4584039561171551, 0.05487954303617039)</td>\n",
       "      <td>(0.5290902094581543, 0.033871154740057634)</td>\n",
       "      <td>(0.5309946890450492, 0.03614756565574484)</td>\n",
       "      <td>(0.5246593700652559, 0.037239205828797695)</td>\n",
       "      <td>(0.40888890514333737, 0.048491773881137505)</td>\n",
       "      <td>(0.4338368870858087, 0.05801723583284577)</td>\n",
       "      <td>(0.5039900979296946, 0.045605301150329916)</td>\n",
       "      <td>(0.5063234592178008, 0.046139432955928)</td>\n",
       "      <td>(0.2253901657494819, 0.06081726954186284)</td>\n",
       "      <td>(0.5283689170284585, 0.03740073019320621)</td>\n",
       "      <td>(0.4482983445822539, 0.055804447852329124)</td>\n",
       "      <td>(0.5241400344377773, 0.039053347725237095)</td>\n",
       "      <td>(0.5270010270018308, 0.04053893275502055)</td>\n",
       "      <td>(0.5257395520202698, 0.04025053986911108)</td>\n",
       "      <td>(0.4337852067994485, 0.05221503110952009)</td>\n",
       "      <td>(0.43195905753680397, 0.05529603330344233)</td>\n",
       "      <td>(0.5025359604078671, 0.04837791651179072)</td>\n",
       "      <td>(0.5032515623797864, 0.04946788422876641)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.40757382369206757, 0.04089046344679707)</td>\n",
       "      <td>(0.602710365146897, 0.012601638494659407)</td>\n",
       "      <td>(0.5482529522731582, 0.017917953608313397)</td>\n",
       "      <td>(0.6004639225803156, 0.012083147765057757)</td>\n",
       "      <td>(0.6019958836080831, 0.012664818216773137)</td>\n",
       "      <td>(0.5998499525870375, 0.012400189157987967)</td>\n",
       "      <td>(0.526178860834034, 0.014629917324643877)</td>\n",
       "      <td>(0.5510985239997457, 0.01705905669584118)</td>\n",
       "      <td>(0.5970409209394866, 0.016258573594492138)</td>\n",
       "      <td>(0.5967962500263246, 0.016070798725203013)</td>\n",
       "      <td>(0.34873693726932026, 0.02449233596576483)</td>\n",
       "      <td>(0.6032035030147566, 0.013104743354055544)</td>\n",
       "      <td>(0.5495740072805873, 0.02037171443110018)</td>\n",
       "      <td>(0.6004060011525232, 0.01242913432287744)</td>\n",
       "      <td>(0.601714916074493, 0.013009121956936506)</td>\n",
       "      <td>(0.5994845470182883, 0.013004558779394184)</td>\n",
       "      <td>(0.550788798773088, 0.016100526304124462)</td>\n",
       "      <td>(0.5546652523823858, 0.016864825891738905)</td>\n",
       "      <td>(0.5982513005708373, 0.01608308268058636)</td>\n",
       "      <td>(0.5968899350888409, 0.016483690681354166)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.6147879761881972, 0.012127927248098469)</td>\n",
       "      <td>(0.6356126920585217, 0.010001930234267594)</td>\n",
       "      <td>(0.6210620445756188, 0.010465137180770707)</td>\n",
       "      <td>(0.6376338461289641, 0.009719830104050144)</td>\n",
       "      <td>(0.6357730860207562, 0.00983426267889582)</td>\n",
       "      <td>(0.6378956891631123, 0.00955237241291143)</td>\n",
       "      <td>(0.5950263531912541, 0.012162522470339322)</td>\n",
       "      <td>(0.6217366959246404, 0.009457708011631934)</td>\n",
       "      <td>(0.6466734790392024, 0.008822389726075769)</td>\n",
       "      <td>(0.6471393891857908, 0.008679161605219695)</td>\n",
       "      <td>(0.5977729307394338, 0.014961067791364543)</td>\n",
       "      <td>(0.6382169807215811, 0.009718471220245054)</td>\n",
       "      <td>(0.6232359889990221, 0.010885534952109019)</td>\n",
       "      <td>(0.6387274164459839, 0.009982753629128878)</td>\n",
       "      <td>(0.6380747581355611, 0.009586542557578301)</td>\n",
       "      <td>(0.6388133615724398, 0.009772566427251037)</td>\n",
       "      <td>(0.6222683066952354, 0.009344901800298407)</td>\n",
       "      <td>(0.6276470222479963, 0.009302832065194456)</td>\n",
       "      <td>(0.6476160083365017, 0.00865959742129311)</td>\n",
       "      <td>(0.647783530674144, 0.008773801937641717)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">class_scene</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.2887297503645147, 0.05862527850388877)</td>\n",
       "      <td>(0.5784668927506504, 0.03446640929819958)</td>\n",
       "      <td>(0.5115300668510798, 0.03256815469944293)</td>\n",
       "      <td>(0.57540760444618, 0.03181567412071789)</td>\n",
       "      <td>(0.5796996787071741, 0.0324164892900818)</td>\n",
       "      <td>(0.5735263567998383, 0.03473425360821763)</td>\n",
       "      <td>(0.5177803094157308, 0.03008997869106008)</td>\n",
       "      <td>(0.5328062149643972, 0.030564692077995996)</td>\n",
       "      <td>(0.5845168388717582, 0.0362940080635773)</td>\n",
       "      <td>(0.5858999930740788, 0.039405997505666354)</td>\n",
       "      <td>(0.30435632088021364, 0.04323775028904248)</td>\n",
       "      <td>(0.5747423871521429, 0.03192019090581552)</td>\n",
       "      <td>(0.5050598819525266, 0.03644012395279655)</td>\n",
       "      <td>(0.5684620634573567, 0.03708232857643755)</td>\n",
       "      <td>(0.57818547892772, 0.03197882795358331)</td>\n",
       "      <td>(0.5727866694244487, 0.03503945449195237)</td>\n",
       "      <td>(0.530233723953394, 0.028906606097908766)</td>\n",
       "      <td>(0.5334327555546071, 0.029681464868250088)</td>\n",
       "      <td>(0.5807618772068895, 0.03937726175203158)</td>\n",
       "      <td>(0.5834022947900864, 0.03768671293738099)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.5089043370203418, 0.04279409255393078)</td>\n",
       "      <td>(0.6739346087943122, 0.014650293124507045)</td>\n",
       "      <td>(0.6090737830923793, 0.019935279065034835)</td>\n",
       "      <td>(0.671925693007461, 0.016182242412259056)</td>\n",
       "      <td>(0.6717871732521119, 0.014057906800437458)</td>\n",
       "      <td>(0.672135398330212, 0.01596534917066078)</td>\n",
       "      <td>(0.601883329050297, 0.016958029481028997)</td>\n",
       "      <td>(0.6350118677715724, 0.021498477857871755)</td>\n",
       "      <td>(0.686308754579231, 0.0189824921310639)</td>\n",
       "      <td>(0.6865918489245781, 0.01955594999628145)</td>\n",
       "      <td>(0.45494208293289146, 0.022712872446578675)</td>\n",
       "      <td>(0.670808487368288, 0.0164827022996175)</td>\n",
       "      <td>(0.6046389774396257, 0.022014523087939476)</td>\n",
       "      <td>(0.6699252752915384, 0.017648460424864235)</td>\n",
       "      <td>(0.6702728473597124, 0.015750736555436785)</td>\n",
       "      <td>(0.6707292138704504, 0.017297915403356297)</td>\n",
       "      <td>(0.6252011800199345, 0.019983130497921765)</td>\n",
       "      <td>(0.639498464251877, 0.020473841625982907)</td>\n",
       "      <td>(0.6855349428914443, 0.018378200142275564)</td>\n",
       "      <td>(0.685654402869865, 0.01885520253441516)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.7213284808260377, 0.010796438135378622)</td>\n",
       "      <td>(0.7470911069823307, 0.00782062708433269)</td>\n",
       "      <td>(0.721051577755413, 0.009488726868769143)</td>\n",
       "      <td>(0.7521490122923703, 0.008048671368619944)</td>\n",
       "      <td>(0.7468382127783063, 0.008040433100185037)</td>\n",
       "      <td>(0.7530424486003381, 0.0077555940540884)</td>\n",
       "      <td>(0.714081771974045, 0.009345271799003962)</td>\n",
       "      <td>(0.7394602727324334, 0.009207807883684718)</td>\n",
       "      <td>(0.7664415288072245, 0.007543460585583579)</td>\n",
       "      <td>(0.7673879590375816, 0.007320098921811779)</td>\n",
       "      <td>(0.7164110889807247, 0.009879127750692874)</td>\n",
       "      <td>(0.7522253299225057, 0.007434724077601798)</td>\n",
       "      <td>(0.726636083178869, 0.00872418679335351)</td>\n",
       "      <td>(0.7540963752747188, 0.007787876039836823)</td>\n",
       "      <td>(0.7525095492669461, 0.007506247579426184)</td>\n",
       "      <td>(0.7553777748805911, 0.007557139660479595)</td>\n",
       "      <td>(0.7298078434307987, 0.00912421093394073)</td>\n",
       "      <td>(0.7417887741876835, 0.009258309235851643)</td>\n",
       "      <td>(0.7669213756693692, 0.007532710986628501)</td>\n",
       "      <td>(0.7676583949519105, 0.0075301177908345274)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">jigsaw</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.2207003257685848, 0.05323758307374539)</td>\n",
       "      <td>(0.46878848443921484, 0.03346631389814326)</td>\n",
       "      <td>(0.41365310656779836, 0.04302183159281723)</td>\n",
       "      <td>(0.46881452540604135, 0.03222358806960093)</td>\n",
       "      <td>(0.46842583340691035, 0.03168698480244446)</td>\n",
       "      <td>(0.46723473604896254, 0.035463327240661244)</td>\n",
       "      <td>(0.38373611869697444, 0.03952735085972693)</td>\n",
       "      <td>(0.3974327984790691, 0.036090275140109505)</td>\n",
       "      <td>(0.4549156573218597, 0.0323777064064323)</td>\n",
       "      <td>(0.45708511720404416, 0.03395570779461837)</td>\n",
       "      <td>(0.19652745286228154, 0.04963540613879445)</td>\n",
       "      <td>(0.4622564384258587, 0.028615359208692175)</td>\n",
       "      <td>(0.37920255038829365, 0.041124005614479424)</td>\n",
       "      <td>(0.45937360277694367, 0.03143768740745111)</td>\n",
       "      <td>(0.46087754836277406, 0.03524206301031955)</td>\n",
       "      <td>(0.4595508362603906, 0.032662387643707665)</td>\n",
       "      <td>(0.3909283955843174, 0.039409730133135264)</td>\n",
       "      <td>(0.3926947645621249, 0.03593172666872009)</td>\n",
       "      <td>(0.44894547314522143, 0.033795221290027676)</td>\n",
       "      <td>(0.4514841629153789, 0.0328045889148291)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.3517823446373656, 0.03442509114435134)</td>\n",
       "      <td>(0.5342663314803187, 0.015143814455023047)</td>\n",
       "      <td>(0.48941665924830624, 0.014772617289250075)</td>\n",
       "      <td>(0.5345303213603912, 0.014979937550821082)</td>\n",
       "      <td>(0.5349870983566377, 0.01618688479532531)</td>\n",
       "      <td>(0.5350536564107555, 0.014806101972822935)</td>\n",
       "      <td>(0.46334605981797167, 0.01711813055359343)</td>\n",
       "      <td>(0.47923468108066436, 0.018250556085689148)</td>\n",
       "      <td>(0.5228440545442976, 0.01811077738703894)</td>\n",
       "      <td>(0.5229222650855266, 0.017853631417326328)</td>\n",
       "      <td>(0.30797065051397765, 0.023253259739534758)</td>\n",
       "      <td>(0.523640882816323, 0.017498452329188827)</td>\n",
       "      <td>(0.46166613348253, 0.020946789398097257)</td>\n",
       "      <td>(0.5231865674234477, 0.016854152612205242)</td>\n",
       "      <td>(0.5250205489591908, 0.016071358721454466)</td>\n",
       "      <td>(0.5244674141564419, 0.016668940452080127)</td>\n",
       "      <td>(0.46932116938359664, 0.017810343956085963)</td>\n",
       "      <td>(0.47319557906762677, 0.017567802441177602)</td>\n",
       "      <td>(0.5193602366283976, 0.018620997131067137)</td>\n",
       "      <td>(0.5187654789077182, 0.018834257261198323)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.5262849274325947, 0.011703967363664781)</td>\n",
       "      <td>(0.5728896416796527, 0.011515808525299097)</td>\n",
       "      <td>(0.555687324015635, 0.012555723511946205)</td>\n",
       "      <td>(0.5772818437534543, 0.010955233596917574)</td>\n",
       "      <td>(0.5740251414497959, 0.011145091539400544)</td>\n",
       "      <td>(0.5787826571371845, 0.010701150442829764)</td>\n",
       "      <td>(0.541875197902433, 0.012785722565071798)</td>\n",
       "      <td>(0.5592775026817454, 0.01333932877520838)</td>\n",
       "      <td>(0.5908807031822614, 0.012020702295734929)</td>\n",
       "      <td>(0.5915957114051692, 0.011686768470246165)</td>\n",
       "      <td>(0.48761866876736903, 0.016231954842181887)</td>\n",
       "      <td>(0.5751258681578658, 0.010809916463776293)</td>\n",
       "      <td>(0.5480395827512623, 0.012108544179377798)</td>\n",
       "      <td>(0.5767782554874958, 0.010681600662265924)</td>\n",
       "      <td>(0.5771608305483729, 0.010644304446166828)</td>\n",
       "      <td>(0.5789213124966097, 0.010708759094510367)</td>\n",
       "      <td>(0.5541877369045086, 0.013295531450939643)</td>\n",
       "      <td>(0.5603151337679675, 0.013098304344293047)</td>\n",
       "      <td>(0.5901621761108548, 0.011846133118834857)</td>\n",
       "      <td>(0.5906822447636347, 0.011297986416360548)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">normal</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.31248589749553973, 0.05627180288658978)</td>\n",
       "      <td>(0.4977435339741423, 0.040063685619245365)</td>\n",
       "      <td>(0.4640695264852453, 0.051572862329892874)</td>\n",
       "      <td>(0.49291980186386675, 0.04214102738486583)</td>\n",
       "      <td>(0.49870669325832767, 0.038271550524586674)</td>\n",
       "      <td>(0.49627655237401647, 0.04211916069259272)</td>\n",
       "      <td>(0.4783629816588173, 0.03698674737185056)</td>\n",
       "      <td>(0.48107339961791046, 0.03829248954890369)</td>\n",
       "      <td>(0.5111550090489944, 0.036051231556209824)</td>\n",
       "      <td>(0.5081634913119872, 0.03443245782942459)</td>\n",
       "      <td>(0.31211971027271007, 0.047942194869371985)</td>\n",
       "      <td>(0.4988061113729704, 0.03634657837085514)</td>\n",
       "      <td>(0.46806791114889934, 0.03818143669315796)</td>\n",
       "      <td>(0.49708478941576734, 0.03648979509090581)</td>\n",
       "      <td>(0.5006877586634295, 0.03564590207891791)</td>\n",
       "      <td>(0.49947040923200137, 0.03681201255776186)</td>\n",
       "      <td>(0.48385686872515327, 0.032847367676860945)</td>\n",
       "      <td>(0.48460839600797906, 0.034309956060331784)</td>\n",
       "      <td>(0.5110197021010497, 0.038297069635865766)</td>\n",
       "      <td>(0.5099238768501747, 0.036489454049932)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.47822487270797714, 0.02945677752265107)</td>\n",
       "      <td>(0.5449881232943214, 0.020287558687811104)</td>\n",
       "      <td>(0.5254580283960391, 0.02760541580379666)</td>\n",
       "      <td>(0.5475067999046171, 0.021522390596687205)</td>\n",
       "      <td>(0.546958751860012, 0.022397171050654606)</td>\n",
       "      <td>(0.550081930035622, 0.022868217441576542)</td>\n",
       "      <td>(0.5251629842035925, 0.02418826636797313)</td>\n",
       "      <td>(0.5378608975065708, 0.02508004817250839)</td>\n",
       "      <td>(0.5526216944086831, 0.021677537859957983)</td>\n",
       "      <td>(0.5547923145379461, 0.022654865971889922)</td>\n",
       "      <td>(0.44013649769957824, 0.03037724900724046)</td>\n",
       "      <td>(0.5449738748069296, 0.019420599105230984)</td>\n",
       "      <td>(0.5239858522255123, 0.024379202528387135)</td>\n",
       "      <td>(0.5456117955491789, 0.019663014468464687)</td>\n",
       "      <td>(0.5485854804419926, 0.0202010812347089)</td>\n",
       "      <td>(0.5504290200847333, 0.01962020335492829)</td>\n",
       "      <td>(0.5346388403363386, 0.024947672838258543)</td>\n",
       "      <td>(0.5389655126681832, 0.026336888461460307)</td>\n",
       "      <td>(0.5528535568074132, 0.02438230537002612)</td>\n",
       "      <td>(0.5546774561380086, 0.024465072691936984)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.5519379025738749, 0.010549910840146528)</td>\n",
       "      <td>(0.5996492874494507, 0.011510492310204434)</td>\n",
       "      <td>(0.5954236511011703, 0.011754447081624885)</td>\n",
       "      <td>(0.6011206381175608, 0.010959670602550526)</td>\n",
       "      <td>(0.609253918056806, 0.011932869015348188)</td>\n",
       "      <td>(0.612095076496638, 0.011330161813595254)</td>\n",
       "      <td>(0.5875575099324175, 0.014588218721210638)</td>\n",
       "      <td>(0.6046377650972282, 0.0145647704162447)</td>\n",
       "      <td>(0.6173958932536151, 0.012154042970956796)</td>\n",
       "      <td>(0.61999840951173, 0.012121929608140769)</td>\n",
       "      <td>(0.5498273051787853, 0.01236742414808791)</td>\n",
       "      <td>(0.5995213924016021, 0.010488058578629562)</td>\n",
       "      <td>(0.5906770382966121, 0.012583959015999822)</td>\n",
       "      <td>(0.600025019828874, 0.010559206659456604)</td>\n",
       "      <td>(0.6109597326452897, 0.011030124241619506)</td>\n",
       "      <td>(0.6121420374742016, 0.011116464002968212)</td>\n",
       "      <td>(0.597961255488895, 0.014559466055023267)</td>\n",
       "      <td>(0.6051229600886052, 0.014073482889382864)</td>\n",
       "      <td>(0.618072805343052, 0.011676727267507044)</td>\n",
       "      <td>(0.6195273763576978, 0.012020018666184747)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">room_layout</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.2003743244970831, 0.07340785723484274)</td>\n",
       "      <td>(0.6705805105609854, 0.03368256522811718)</td>\n",
       "      <td>(0.5858499861726851, 0.048949320850260915)</td>\n",
       "      <td>(0.6629580077722379, 0.035493754631249035)</td>\n",
       "      <td>(0.6705697025484915, 0.032585720941840195)</td>\n",
       "      <td>(0.6658979659464574, 0.032408289612569925)</td>\n",
       "      <td>(0.6087427065214883, 0.0645362297713436)</td>\n",
       "      <td>(0.6128406432297308, 0.0619796901834782)</td>\n",
       "      <td>(0.6623412312883274, 0.047793081485021455)</td>\n",
       "      <td>(0.6674958372162888, 0.04035349857033007)</td>\n",
       "      <td>(0.2289154995998661, 0.08070415241431686)</td>\n",
       "      <td>(0.6645613346526998, 0.03601895254796587)</td>\n",
       "      <td>(0.5963875954176232, 0.048406503092157875)</td>\n",
       "      <td>(0.6628003308046095, 0.0357705043691758)</td>\n",
       "      <td>(0.6696255951449089, 0.03383196658398398)</td>\n",
       "      <td>(0.6713404891309541, 0.03401532026341905)</td>\n",
       "      <td>(0.6190032797314289, 0.06012084973592932)</td>\n",
       "      <td>(0.608976095110317, 0.06740619535812066)</td>\n",
       "      <td>(0.6592657074393751, 0.047787472356045115)</td>\n",
       "      <td>(0.6617332420541168, 0.04579184113146122)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.3634489743038373, 0.04303328813326627)</td>\n",
       "      <td>(0.7368170790908475, 0.013884429953786278)</td>\n",
       "      <td>(0.6793149583168717, 0.01682583020500818)</td>\n",
       "      <td>(0.7359585154589819, 0.014406907255428895)</td>\n",
       "      <td>(0.7389297314652817, 0.014511268601272907)</td>\n",
       "      <td>(0.7380058918263992, 0.014189608629808122)</td>\n",
       "      <td>(0.7172249645852138, 0.013790272021980058)</td>\n",
       "      <td>(0.7221786233633086, 0.014204201936552202)</td>\n",
       "      <td>(0.7460444496563737, 0.01629174836974557)</td>\n",
       "      <td>(0.7460880840158899, 0.016170165920271658)</td>\n",
       "      <td>(0.3531740808431184, 0.029978304635556362)</td>\n",
       "      <td>(0.7359725411190974, 0.01348821435491303)</td>\n",
       "      <td>(0.6981386569970621, 0.02163899959925311)</td>\n",
       "      <td>(0.7353166928506947, 0.014270040203067772)</td>\n",
       "      <td>(0.7386328110812345, 0.013727926511784809)</td>\n",
       "      <td>(0.7375858540196958, 0.013401284988086756)</td>\n",
       "      <td>(0.7214670652736419, 0.014405493707529379)</td>\n",
       "      <td>(0.7224286439903009, 0.014310157368571434)</td>\n",
       "      <td>(0.7454628130122778, 0.016456975284451652)</td>\n",
       "      <td>(0.7454857111728346, 0.016361839347127632)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.6669838762548757, 0.036007320934218566)</td>\n",
       "      <td>(0.7835191446919133, 0.006349865398712821)</td>\n",
       "      <td>(0.7702969907296794, 0.008159876389440601)</td>\n",
       "      <td>(0.7850144602338496, 0.005915337435841312)</td>\n",
       "      <td>(0.7859839439607089, 0.0064063246404699155)</td>\n",
       "      <td>(0.787715042964706, 0.0059314957523752985)</td>\n",
       "      <td>(0.7753954937162455, 0.007572604612528358)</td>\n",
       "      <td>(0.7819658457783478, 0.007313403721504526)</td>\n",
       "      <td>(0.7948450315489325, 0.007248031596692311)</td>\n",
       "      <td>(0.795128647274566, 0.006998035882186933)</td>\n",
       "      <td>(0.645332887764259, 0.04683310227848009)</td>\n",
       "      <td>(0.785568934396315, 0.005907886670948867)</td>\n",
       "      <td>(0.7750273690461202, 0.008200062443555842)</td>\n",
       "      <td>(0.7854891247159413, 0.005885005141664014)</td>\n",
       "      <td>(0.7881254921959038, 0.006002280944310943)</td>\n",
       "      <td>(0.7881225450867363, 0.005656792614351427)</td>\n",
       "      <td>(0.7814846922465093, 0.007260365567616939)</td>\n",
       "      <td>(0.7828103085834336, 0.007368187530498692)</td>\n",
       "      <td>(0.7944885103038763, 0.007419901751883567)</td>\n",
       "      <td>(0.7948384467148975, 0.007175565649856891)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">segmentsemantic</th>\n",
       "      <th>32</th>\n",
       "      <td>(0.4192804517829179, 0.06579232223508408)</td>\n",
       "      <td>(0.6804321732028368, 0.03048428711224278)</td>\n",
       "      <td>(0.6247357601057743, 0.025988968383377456)</td>\n",
       "      <td>(0.6829026454093337, 0.029946130611240497)</td>\n",
       "      <td>(0.6798026838221365, 0.03176582621932754)</td>\n",
       "      <td>(0.6840845392237699, 0.030205858089888668)</td>\n",
       "      <td>(0.6222468456254525, 0.02649644268963176)</td>\n",
       "      <td>(0.6417843851185481, 0.03245392605503887)</td>\n",
       "      <td>(0.6816319131126121, 0.027667408920111304)</td>\n",
       "      <td>(0.6843335335413395, 0.0290114600484994)</td>\n",
       "      <td>(0.4532760007321898, 0.04911838082201868)</td>\n",
       "      <td>(0.678739257439977, 0.0321291053263623)</td>\n",
       "      <td>(0.6304712274419465, 0.025113285663671776)</td>\n",
       "      <td>(0.6810052802345641, 0.029921154224027247)</td>\n",
       "      <td>(0.679907067749279, 0.03044064402737666)</td>\n",
       "      <td>(0.682456469491851, 0.03036952087945571)</td>\n",
       "      <td>(0.6292784087356239, 0.023335146771729813)</td>\n",
       "      <td>(0.6439872554061616, 0.03039597374639378)</td>\n",
       "      <td>(0.6801256552654428, 0.02897394043617278)</td>\n",
       "      <td>(0.6839799342007915, 0.02884933558804223)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>(0.6404054550507564, 0.02862123074387072)</td>\n",
       "      <td>(0.7499688279553025, 0.0155434178957461)</td>\n",
       "      <td>(0.6939212516660109, 0.014015909641323272)</td>\n",
       "      <td>(0.7538923082074419, 0.015997204481106142)</td>\n",
       "      <td>(0.7445078348521852, 0.0161234727754742)</td>\n",
       "      <td>(0.7503354508683213, 0.01663981222587707)</td>\n",
       "      <td>(0.68219774460563, 0.01395157906968609)</td>\n",
       "      <td>(0.7100872807370512, 0.015920942137977807)</td>\n",
       "      <td>(0.7426547902520985, 0.013738896877968795)</td>\n",
       "      <td>(0.7473596266393223, 0.015051941578942213)</td>\n",
       "      <td>(0.6056522893030151, 0.029740940892274605)</td>\n",
       "      <td>(0.7503943775361747, 0.01582071531038657)</td>\n",
       "      <td>(0.7014360705380465, 0.01660833608506841)</td>\n",
       "      <td>(0.7537509324844437, 0.016233374104689554)</td>\n",
       "      <td>(0.7450750653511455, 0.017057773282356154)</td>\n",
       "      <td>(0.7499200758109204, 0.017355749859086413)</td>\n",
       "      <td>(0.6944363287352295, 0.012964961688379559)</td>\n",
       "      <td>(0.7124823781194742, 0.015139755166673992)</td>\n",
       "      <td>(0.7434961890196964, 0.013824914519730816)</td>\n",
       "      <td>(0.7470673111344119, 0.015078729833010401)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>(0.7900918495825144, 0.00603177777432193)</td>\n",
       "      <td>(0.8105024536284688, 0.0056757266692478514)</td>\n",
       "      <td>(0.7814896609515775, 0.007986381146106046)</td>\n",
       "      <td>(0.8182399816741959, 0.00569515357029767)</td>\n",
       "      <td>(0.8102442574604289, 0.00568283430918751)</td>\n",
       "      <td>(0.8187212160638498, 0.0058384066064837556)</td>\n",
       "      <td>(0.736127742496538, 0.007831516326493768)</td>\n",
       "      <td>(0.7709479020653969, 0.006704243860357273)</td>\n",
       "      <td>(0.8009848900762353, 0.0070137156797390075)</td>\n",
       "      <td>(0.8083014941809477, 0.006666437399595372)</td>\n",
       "      <td>(0.7942909108330418, 0.0059261463269096575)</td>\n",
       "      <td>(0.8141762129376804, 0.00606547994774946)</td>\n",
       "      <td>(0.7941824847138443, 0.006695891589384316)</td>\n",
       "      <td>(0.8190825741940148, 0.006147613539247302)</td>\n",
       "      <td>(0.8138836025275854, 0.006315852640165273)</td>\n",
       "      <td>(0.8198029511964865, 0.00614854819364408)</td>\n",
       "      <td>(0.7538201468004067, 0.00758376566663289)</td>\n",
       "      <td>(0.7751509585510337, 0.006838355809522419)</td>\n",
       "      <td>(0.8034090356896613, 0.006864482881829786)</td>\n",
       "      <td>(0.8095243458987124, 0.00629157246076777)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                Onehot  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32          (0.42131337433001564, 0.05624899815157225)   \n",
       "                128          (0.5482058211543945, 0.01870653762409465)   \n",
       "                1024        (0.6036222576886772, 0.008387574631852848)   \n",
       "class_object    32          (0.23489499032400288, 0.05896682705600237)   \n",
       "                128         (0.40757382369206757, 0.04089046344679707)   \n",
       "                1024        (0.6147879761881972, 0.012127927248098469)   \n",
       "class_scene     32           (0.2887297503645147, 0.05862527850388877)   \n",
       "                128          (0.5089043370203418, 0.04279409255393078)   \n",
       "                1024        (0.7213284808260377, 0.010796438135378622)   \n",
       "jigsaw          32           (0.2207003257685848, 0.05323758307374539)   \n",
       "                128          (0.3517823446373656, 0.03442509114435134)   \n",
       "                1024        (0.5262849274325947, 0.011703967363664781)   \n",
       "normal          32          (0.31248589749553973, 0.05627180288658978)   \n",
       "                128         (0.47822487270797714, 0.02945677752265107)   \n",
       "                1024        (0.5519379025738749, 0.010549910840146528)   \n",
       "room_layout     32           (0.2003743244970831, 0.07340785723484274)   \n",
       "                128          (0.3634489743038373, 0.04303328813326627)   \n",
       "                1024        (0.6669838762548757, 0.036007320934218566)   \n",
       "segmentsemantic 32           (0.4192804517829179, 0.06579232223508408)   \n",
       "                128          (0.6404054550507564, 0.02862123074387072)   \n",
       "                1024         (0.7900918495825144, 0.00603177777432193)   \n",
       "\n",
       "                                                                   GRAF  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32           (0.5568433408681017, 0.030300641200930966)   \n",
       "                128          (0.5944363524344953, 0.012474771538213879)   \n",
       "                1024         (0.6267138770313302, 0.008365342202978267)   \n",
       "class_object    32             (0.5326160647975239, 0.0346813651725688)   \n",
       "                128           (0.602710365146897, 0.012601638494659407)   \n",
       "                1024         (0.6356126920585217, 0.010001930234267594)   \n",
       "class_scene     32            (0.5784668927506504, 0.03446640929819958)   \n",
       "                128          (0.6739346087943122, 0.014650293124507045)   \n",
       "                1024          (0.7470911069823307, 0.00782062708433269)   \n",
       "jigsaw          32           (0.46878848443921484, 0.03346631389814326)   \n",
       "                128          (0.5342663314803187, 0.015143814455023047)   \n",
       "                1024         (0.5728896416796527, 0.011515808525299097)   \n",
       "normal          32           (0.4977435339741423, 0.040063685619245365)   \n",
       "                128          (0.5449881232943214, 0.020287558687811104)   \n",
       "                1024         (0.5996492874494507, 0.011510492310204434)   \n",
       "room_layout     32            (0.6705805105609854, 0.03368256522811718)   \n",
       "                128          (0.7368170790908475, 0.013884429953786278)   \n",
       "                1024         (0.7835191446919133, 0.006349865398712821)   \n",
       "segmentsemantic 32            (0.6804321732028368, 0.03048428711224278)   \n",
       "                128            (0.7499688279553025, 0.0155434178957461)   \n",
       "                1024        (0.8105024536284688, 0.0056757266692478514)   \n",
       "\n",
       "                                                           Onehot + F&P  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32           (0.39433518325506883, 0.06529893856507621)   \n",
       "                128          (0.5423329430413283, 0.027434560716485618)   \n",
       "                1024         (0.6039797392488457, 0.009442367139994115)   \n",
       "class_object    32            (0.4584039561171551, 0.05487954303617039)   \n",
       "                128          (0.5482529522731582, 0.017917953608313397)   \n",
       "                1024         (0.6210620445756188, 0.010465137180770707)   \n",
       "class_scene     32            (0.5115300668510798, 0.03256815469944293)   \n",
       "                128          (0.6090737830923793, 0.019935279065034835)   \n",
       "                1024          (0.721051577755413, 0.009488726868769143)   \n",
       "jigsaw          32           (0.41365310656779836, 0.04302183159281723)   \n",
       "                128         (0.48941665924830624, 0.014772617289250075)   \n",
       "                1024          (0.555687324015635, 0.012555723511946205)   \n",
       "normal          32           (0.4640695264852453, 0.051572862329892874)   \n",
       "                128           (0.5254580283960391, 0.02760541580379666)   \n",
       "                1024         (0.5954236511011703, 0.011754447081624885)   \n",
       "room_layout     32           (0.5858499861726851, 0.048949320850260915)   \n",
       "                128           (0.6793149583168717, 0.01682583020500818)   \n",
       "                1024         (0.7702969907296794, 0.008159876389440601)   \n",
       "segmentsemantic 32           (0.6247357601057743, 0.025988968383377456)   \n",
       "                128          (0.6939212516660109, 0.014015909641323272)   \n",
       "                1024         (0.7814896609515775, 0.007986381146106046)   \n",
       "\n",
       "                                                         GRAF + Onehot  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32           (0.553655337854133, 0.029414119525018705)   \n",
       "                128          (0.5936217657827685, 0.01284198748412974)   \n",
       "                1024        (0.6270962782023536, 0.008405671537546353)   \n",
       "class_object    32          (0.5290902094581543, 0.033871154740057634)   \n",
       "                128         (0.6004639225803156, 0.012083147765057757)   \n",
       "                1024        (0.6376338461289641, 0.009719830104050144)   \n",
       "class_scene     32             (0.57540760444618, 0.03181567412071789)   \n",
       "                128          (0.671925693007461, 0.016182242412259056)   \n",
       "                1024        (0.7521490122923703, 0.008048671368619944)   \n",
       "jigsaw          32          (0.46881452540604135, 0.03222358806960093)   \n",
       "                128         (0.5345303213603912, 0.014979937550821082)   \n",
       "                1024        (0.5772818437534543, 0.010955233596917574)   \n",
       "normal          32          (0.49291980186386675, 0.04214102738486583)   \n",
       "                128         (0.5475067999046171, 0.021522390596687205)   \n",
       "                1024        (0.6011206381175608, 0.010959670602550526)   \n",
       "room_layout     32          (0.6629580077722379, 0.035493754631249035)   \n",
       "                128         (0.7359585154589819, 0.014406907255428895)   \n",
       "                1024        (0.7850144602338496, 0.005915337435841312)   \n",
       "segmentsemantic 32          (0.6829026454093337, 0.029946130611240497)   \n",
       "                128         (0.7538923082074419, 0.015997204481106142)   \n",
       "                1024         (0.8182399816741959, 0.00569515357029767)   \n",
       "\n",
       "                                                             GRAF + F&P  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32           (0.5575116476375594, 0.029004935728709654)   \n",
       "                128          (0.5948304905730829, 0.013222041513842856)   \n",
       "                1024           (0.62647923987218, 0.008543311248717395)   \n",
       "class_object    32            (0.5309946890450492, 0.03614756565574484)   \n",
       "                128          (0.6019958836080831, 0.012664818216773137)   \n",
       "                1024          (0.6357730860207562, 0.00983426267889582)   \n",
       "class_scene     32             (0.5796996787071741, 0.0324164892900818)   \n",
       "                128          (0.6717871732521119, 0.014057906800437458)   \n",
       "                1024         (0.7468382127783063, 0.008040433100185037)   \n",
       "jigsaw          32           (0.46842583340691035, 0.03168698480244446)   \n",
       "                128           (0.5349870983566377, 0.01618688479532531)   \n",
       "                1024         (0.5740251414497959, 0.011145091539400544)   \n",
       "normal          32          (0.49870669325832767, 0.038271550524586674)   \n",
       "                128           (0.546958751860012, 0.022397171050654606)   \n",
       "                1024          (0.609253918056806, 0.011932869015348188)   \n",
       "room_layout     32           (0.6705697025484915, 0.032585720941840195)   \n",
       "                128          (0.7389297314652817, 0.014511268601272907)   \n",
       "                1024        (0.7859839439607089, 0.0064063246404699155)   \n",
       "segmentsemantic 32            (0.6798026838221365, 0.03176582621932754)   \n",
       "                128            (0.7445078348521852, 0.0161234727754742)   \n",
       "                1024          (0.8102442574604289, 0.00568283430918751)   \n",
       "\n",
       "                                                    GRAF + Onehot + F&P  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32           (0.5532510679660605, 0.030044062037441307)   \n",
       "                128           (0.593992878519113, 0.013375181070811046)   \n",
       "                1024          (0.626283196877164, 0.008199463900764729)   \n",
       "class_object    32           (0.5246593700652559, 0.037239205828797695)   \n",
       "                128          (0.5998499525870375, 0.012400189157987967)   \n",
       "                1024          (0.6378956891631123, 0.00955237241291143)   \n",
       "class_scene     32            (0.5735263567998383, 0.03473425360821763)   \n",
       "                128            (0.672135398330212, 0.01596534917066078)   \n",
       "                1024           (0.7530424486003381, 0.0077555940540884)   \n",
       "jigsaw          32          (0.46723473604896254, 0.035463327240661244)   \n",
       "                128          (0.5350536564107555, 0.014806101972822935)   \n",
       "                1024         (0.5787826571371845, 0.010701150442829764)   \n",
       "normal          32           (0.49627655237401647, 0.04211916069259272)   \n",
       "                128           (0.550081930035622, 0.022868217441576542)   \n",
       "                1024          (0.612095076496638, 0.011330161813595254)   \n",
       "room_layout     32           (0.6658979659464574, 0.032408289612569925)   \n",
       "                128          (0.7380058918263992, 0.014189608629808122)   \n",
       "                1024         (0.787715042964706, 0.0059314957523752985)   \n",
       "segmentsemantic 32           (0.6840845392237699, 0.030205858089888668)   \n",
       "                128           (0.7503354508683213, 0.01663981222587707)   \n",
       "                1024        (0.8187212160638498, 0.0058384066064837556)   \n",
       "\n",
       "                                                              ZCP + F&P  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32          (0.45902171002177083, 0.038039408604563354)   \n",
       "                128           (0.5194929287527035, 0.01889628174164421)   \n",
       "                1024         (0.5587452757035001, 0.009782961913005166)   \n",
       "class_object    32          (0.40888890514333737, 0.048491773881137505)   \n",
       "                128           (0.526178860834034, 0.014629917324643877)   \n",
       "                1024         (0.5950263531912541, 0.012162522470339322)   \n",
       "class_scene     32            (0.5177803094157308, 0.03008997869106008)   \n",
       "                128           (0.601883329050297, 0.016958029481028997)   \n",
       "                1024          (0.714081771974045, 0.009345271799003962)   \n",
       "jigsaw          32           (0.38373611869697444, 0.03952735085972693)   \n",
       "                128          (0.46334605981797167, 0.01711813055359343)   \n",
       "                1024          (0.541875197902433, 0.012785722565071798)   \n",
       "normal          32            (0.4783629816588173, 0.03698674737185056)   \n",
       "                128           (0.5251629842035925, 0.02418826636797313)   \n",
       "                1024         (0.5875575099324175, 0.014588218721210638)   \n",
       "room_layout     32             (0.6087427065214883, 0.0645362297713436)   \n",
       "                128          (0.7172249645852138, 0.013790272021980058)   \n",
       "                1024         (0.7753954937162455, 0.007572604612528358)   \n",
       "segmentsemantic 32            (0.6222468456254525, 0.02649644268963176)   \n",
       "                128             (0.68219774460563, 0.01395157906968609)   \n",
       "                1024          (0.736127742496538, 0.007831516326493768)   \n",
       "\n",
       "                                                     ZCP + Onehot + F&P  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32           (0.48362247774588213, 0.04267248362902128)   \n",
       "                128           (0.5547845812549399, 0.01886557435564282)   \n",
       "                1024          (0.5988029592890858, 0.00877315854391037)   \n",
       "class_object    32            (0.4338368870858087, 0.05801723583284577)   \n",
       "                128           (0.5510985239997457, 0.01705905669584118)   \n",
       "                1024         (0.6217366959246404, 0.009457708011631934)   \n",
       "class_scene     32           (0.5328062149643972, 0.030564692077995996)   \n",
       "                128          (0.6350118677715724, 0.021498477857871755)   \n",
       "                1024         (0.7394602727324334, 0.009207807883684718)   \n",
       "jigsaw          32           (0.3974327984790691, 0.036090275140109505)   \n",
       "                128         (0.47923468108066436, 0.018250556085689148)   \n",
       "                1024          (0.5592775026817454, 0.01333932877520838)   \n",
       "normal          32           (0.48107339961791046, 0.03829248954890369)   \n",
       "                128           (0.5378608975065708, 0.02508004817250839)   \n",
       "                1024           (0.6046377650972282, 0.0145647704162447)   \n",
       "room_layout     32             (0.6128406432297308, 0.0619796901834782)   \n",
       "                128          (0.7221786233633086, 0.014204201936552202)   \n",
       "                1024         (0.7819658457783478, 0.007313403721504526)   \n",
       "segmentsemantic 32            (0.6417843851185481, 0.03245392605503887)   \n",
       "                128          (0.7100872807370512, 0.015920942137977807)   \n",
       "                1024         (0.7709479020653969, 0.006704243860357273)   \n",
       "\n",
       "                                                       ZCP + GRAF + F&P  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32           (0.5533373323824581, 0.033489260754819755)   \n",
       "                128            (0.595638121785306, 0.01210445357323343)   \n",
       "                1024          (0.634198794312162, 0.007672630827658077)   \n",
       "class_object    32           (0.5039900979296946, 0.045605301150329916)   \n",
       "                128          (0.5970409209394866, 0.016258573594492138)   \n",
       "                1024         (0.6466734790392024, 0.008822389726075769)   \n",
       "class_scene     32             (0.5845168388717582, 0.0362940080635773)   \n",
       "                128             (0.686308754579231, 0.0189824921310639)   \n",
       "                1024         (0.7664415288072245, 0.007543460585583579)   \n",
       "jigsaw          32             (0.4549156573218597, 0.0323777064064323)   \n",
       "                128           (0.5228440545442976, 0.01811077738703894)   \n",
       "                1024         (0.5908807031822614, 0.012020702295734929)   \n",
       "normal          32           (0.5111550090489944, 0.036051231556209824)   \n",
       "                128          (0.5526216944086831, 0.021677537859957983)   \n",
       "                1024         (0.6173958932536151, 0.012154042970956796)   \n",
       "room_layout     32           (0.6623412312883274, 0.047793081485021455)   \n",
       "                128           (0.7460444496563737, 0.01629174836974557)   \n",
       "                1024         (0.7948450315489325, 0.007248031596692311)   \n",
       "segmentsemantic 32           (0.6816319131126121, 0.027667408920111304)   \n",
       "                128          (0.7426547902520985, 0.013738896877968795)   \n",
       "                1024        (0.8009848900762353, 0.0070137156797390075)   \n",
       "\n",
       "                                                            Everything  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32           (0.5526703464827988, 0.02955107522111283)   \n",
       "                128          (0.5956919012198889, 0.01178133291248549)   \n",
       "                1024        (0.6340426877289199, 0.008239493572385605)   \n",
       "class_object    32             (0.5063234592178008, 0.046139432955928)   \n",
       "                128         (0.5967962500263246, 0.016070798725203013)   \n",
       "                1024        (0.6471393891857908, 0.008679161605219695)   \n",
       "class_scene     32          (0.5858999930740788, 0.039405997505666354)   \n",
       "                128          (0.6865918489245781, 0.01955594999628145)   \n",
       "                1024        (0.7673879590375816, 0.007320098921811779)   \n",
       "jigsaw          32          (0.45708511720404416, 0.03395570779461837)   \n",
       "                128         (0.5229222650855266, 0.017853631417326328)   \n",
       "                1024        (0.5915957114051692, 0.011686768470246165)   \n",
       "normal          32           (0.5081634913119872, 0.03443245782942459)   \n",
       "                128         (0.5547923145379461, 0.022654865971889922)   \n",
       "                1024          (0.61999840951173, 0.012121929608140769)   \n",
       "room_layout     32           (0.6674958372162888, 0.04035349857033007)   \n",
       "                128         (0.7460880840158899, 0.016170165920271658)   \n",
       "                1024         (0.795128647274566, 0.006998035882186933)   \n",
       "segmentsemantic 32            (0.6843335335413395, 0.0290114600484994)   \n",
       "                128         (0.7473596266393223, 0.015051941578942213)   \n",
       "                1024        (0.8083014941809477, 0.006666437399595372)   \n",
       "\n",
       "                                                            Onehot + PE  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32             (0.4657229988167805, 0.0610124357603322)   \n",
       "                128          (0.5437331092948466, 0.015448014912039511)   \n",
       "                1024         (0.5983618432677335, 0.009192550105327523)   \n",
       "class_object    32            (0.2253901657494819, 0.06081726954186284)   \n",
       "                128          (0.34873693726932026, 0.02449233596576483)   \n",
       "                1024         (0.5977729307394338, 0.014961067791364543)   \n",
       "class_scene     32           (0.30435632088021364, 0.04323775028904248)   \n",
       "                128         (0.45494208293289146, 0.022712872446578675)   \n",
       "                1024         (0.7164110889807247, 0.009879127750692874)   \n",
       "jigsaw          32           (0.19652745286228154, 0.04963540613879445)   \n",
       "                128         (0.30797065051397765, 0.023253259739534758)   \n",
       "                1024        (0.48761866876736903, 0.016231954842181887)   \n",
       "normal          32          (0.31211971027271007, 0.047942194869371985)   \n",
       "                128          (0.44013649769957824, 0.03037724900724046)   \n",
       "                1024          (0.5498273051787853, 0.01236742414808791)   \n",
       "room_layout     32            (0.2289154995998661, 0.08070415241431686)   \n",
       "                128          (0.3531740808431184, 0.029978304635556362)   \n",
       "                1024           (0.645332887764259, 0.04683310227848009)   \n",
       "segmentsemantic 32            (0.4532760007321898, 0.04911838082201868)   \n",
       "                128          (0.6056522893030151, 0.029740940892274605)   \n",
       "                1024        (0.7942909108330418, 0.0059261463269096575)   \n",
       "\n",
       "                                                             GRAF + PE  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32          (0.5533214376421388, 0.030054728205799568)   \n",
       "                128         (0.5893149861686449, 0.013009391508265054)   \n",
       "                1024        (0.6241689941980385, 0.008486635985324287)   \n",
       "class_object    32           (0.5283689170284585, 0.03740073019320621)   \n",
       "                128         (0.6032035030147566, 0.013104743354055544)   \n",
       "                1024        (0.6382169807215811, 0.009718471220245054)   \n",
       "class_scene     32           (0.5747423871521429, 0.03192019090581552)   \n",
       "                128            (0.670808487368288, 0.0164827022996175)   \n",
       "                1024        (0.7522253299225057, 0.007434724077601798)   \n",
       "jigsaw          32          (0.4622564384258587, 0.028615359208692175)   \n",
       "                128          (0.523640882816323, 0.017498452329188827)   \n",
       "                1024        (0.5751258681578658, 0.010809916463776293)   \n",
       "normal          32           (0.4988061113729704, 0.03634657837085514)   \n",
       "                128         (0.5449738748069296, 0.019420599105230984)   \n",
       "                1024        (0.5995213924016021, 0.010488058578629562)   \n",
       "room_layout     32           (0.6645613346526998, 0.03601895254796587)   \n",
       "                128          (0.7359725411190974, 0.01348821435491303)   \n",
       "                1024         (0.785568934396315, 0.005907886670948867)   \n",
       "segmentsemantic 32             (0.678739257439977, 0.0321291053263623)   \n",
       "                128          (0.7503943775361747, 0.01582071531038657)   \n",
       "                1024         (0.8141762129376804, 0.00606547994774946)   \n",
       "\n",
       "                                                      Onehot + F&P + PE  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32            (0.45361390807812696, 0.0671142288987981)   \n",
       "                128          (0.5558024887152915, 0.017534203532812948)   \n",
       "                1024         (0.6077455159242197, 0.009736433400369025)   \n",
       "class_object    32           (0.4482983445822539, 0.055804447852329124)   \n",
       "                128           (0.5495740072805873, 0.02037171443110018)   \n",
       "                1024         (0.6232359889990221, 0.010885534952109019)   \n",
       "class_scene     32            (0.5050598819525266, 0.03644012395279655)   \n",
       "                128          (0.6046389774396257, 0.022014523087939476)   \n",
       "                1024           (0.726636083178869, 0.00872418679335351)   \n",
       "jigsaw          32          (0.37920255038829365, 0.041124005614479424)   \n",
       "                128            (0.46166613348253, 0.020946789398097257)   \n",
       "                1024         (0.5480395827512623, 0.012108544179377798)   \n",
       "normal          32           (0.46806791114889934, 0.03818143669315796)   \n",
       "                128          (0.5239858522255123, 0.024379202528387135)   \n",
       "                1024         (0.5906770382966121, 0.012583959015999822)   \n",
       "room_layout     32           (0.5963875954176232, 0.048406503092157875)   \n",
       "                128           (0.6981386569970621, 0.02163899959925311)   \n",
       "                1024         (0.7750273690461202, 0.008200062443555842)   \n",
       "segmentsemantic 32           (0.6304712274419465, 0.025113285663671776)   \n",
       "                128           (0.7014360705380465, 0.01660833608506841)   \n",
       "                1024         (0.7941824847138443, 0.006695891589384316)   \n",
       "\n",
       "                                                    GRAF + Onehot + PE  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32          (0.5510717813230754, 0.027895472900277888)   \n",
       "                128         (0.5881224434580681, 0.013871963829607196)   \n",
       "                1024        (0.6230323857648686, 0.008465994736139335)   \n",
       "class_object    32          (0.5241400344377773, 0.039053347725237095)   \n",
       "                128          (0.6004060011525232, 0.01242913432287744)   \n",
       "                1024        (0.6387274164459839, 0.009982753629128878)   \n",
       "class_scene     32           (0.5684620634573567, 0.03708232857643755)   \n",
       "                128         (0.6699252752915384, 0.017648460424864235)   \n",
       "                1024        (0.7540963752747188, 0.007787876039836823)   \n",
       "jigsaw          32          (0.45937360277694367, 0.03143768740745111)   \n",
       "                128         (0.5231865674234477, 0.016854152612205242)   \n",
       "                1024        (0.5767782554874958, 0.010681600662265924)   \n",
       "normal          32          (0.49708478941576734, 0.03648979509090581)   \n",
       "                128         (0.5456117955491789, 0.019663014468464687)   \n",
       "                1024         (0.600025019828874, 0.010559206659456604)   \n",
       "room_layout     32            (0.6628003308046095, 0.0357705043691758)   \n",
       "                128         (0.7353166928506947, 0.014270040203067772)   \n",
       "                1024        (0.7854891247159413, 0.005885005141664014)   \n",
       "segmentsemantic 32          (0.6810052802345641, 0.029921154224027247)   \n",
       "                128         (0.7537509324844437, 0.016233374104689554)   \n",
       "                1024        (0.8190825741940148, 0.006147613539247302)   \n",
       "\n",
       "                                                       GRAF + F&P + PE  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32          (0.5523382277742221, 0.031192354048323306)   \n",
       "                128         (0.5904851821368915, 0.013329946933022637)   \n",
       "                1024        (0.6254497764138837, 0.008771292416416426)   \n",
       "class_object    32           (0.5270010270018308, 0.04053893275502055)   \n",
       "                128          (0.601714916074493, 0.013009121956936506)   \n",
       "                1024        (0.6380747581355611, 0.009586542557578301)   \n",
       "class_scene     32             (0.57818547892772, 0.03197882795358331)   \n",
       "                128         (0.6702728473597124, 0.015750736555436785)   \n",
       "                1024        (0.7525095492669461, 0.007506247579426184)   \n",
       "jigsaw          32          (0.46087754836277406, 0.03524206301031955)   \n",
       "                128         (0.5250205489591908, 0.016071358721454466)   \n",
       "                1024        (0.5771608305483729, 0.010644304446166828)   \n",
       "normal          32           (0.5006877586634295, 0.03564590207891791)   \n",
       "                128           (0.5485854804419926, 0.0202010812347089)   \n",
       "                1024        (0.6109597326452897, 0.011030124241619506)   \n",
       "room_layout     32           (0.6696255951449089, 0.03383196658398398)   \n",
       "                128         (0.7386328110812345, 0.013727926511784809)   \n",
       "                1024        (0.7881254921959038, 0.006002280944310943)   \n",
       "segmentsemantic 32            (0.679907067749279, 0.03044064402737666)   \n",
       "                128         (0.7450750653511455, 0.017057773282356154)   \n",
       "                1024        (0.8138836025275854, 0.006315852640165273)   \n",
       "\n",
       "                                              GRAF + Onehot + F&P + PE  \\\n",
       "dataset         train_size                                               \n",
       "autoencoder     32          (0.5515506587858673, 0.029107430036449295)   \n",
       "                128         (0.5891157556368344, 0.013927047940578976)   \n",
       "                1024        (0.6240078876287803, 0.008650943356647815)   \n",
       "class_object    32           (0.5257395520202698, 0.04025053986911108)   \n",
       "                128         (0.5994845470182883, 0.013004558779394184)   \n",
       "                1024        (0.6388133615724398, 0.009772566427251037)   \n",
       "class_scene     32           (0.5727866694244487, 0.03503945449195237)   \n",
       "                128         (0.6707292138704504, 0.017297915403356297)   \n",
       "                1024        (0.7553777748805911, 0.007557139660479595)   \n",
       "jigsaw          32          (0.4595508362603906, 0.032662387643707665)   \n",
       "                128         (0.5244674141564419, 0.016668940452080127)   \n",
       "                1024        (0.5789213124966097, 0.010708759094510367)   \n",
       "normal          32          (0.49947040923200137, 0.03681201255776186)   \n",
       "                128          (0.5504290200847333, 0.01962020335492829)   \n",
       "                1024        (0.6121420374742016, 0.011116464002968212)   \n",
       "room_layout     32           (0.6713404891309541, 0.03401532026341905)   \n",
       "                128         (0.7375858540196958, 0.013401284988086756)   \n",
       "                1024        (0.7881225450867363, 0.005656792614351427)   \n",
       "segmentsemantic 32            (0.682456469491851, 0.03036952087945571)   \n",
       "                128         (0.7499200758109204, 0.017355749859086413)   \n",
       "                1024         (0.8198029511964865, 0.00614854819364408)   \n",
       "\n",
       "                                                         ZCP + F&P + PE  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32          (0.49691059426164785, 0.044916351688701533)   \n",
       "                128            (0.568195051412126, 0.02138848052055027)   \n",
       "                1024         (0.6123098392809944, 0.007557337460666202)   \n",
       "class_object    32            (0.4337852067994485, 0.05221503110952009)   \n",
       "                128           (0.550788798773088, 0.016100526304124462)   \n",
       "                1024         (0.6222683066952354, 0.009344901800298407)   \n",
       "class_scene     32            (0.530233723953394, 0.028906606097908766)   \n",
       "                128          (0.6252011800199345, 0.019983130497921765)   \n",
       "                1024          (0.7298078434307987, 0.00912421093394073)   \n",
       "jigsaw          32           (0.3909283955843174, 0.039409730133135264)   \n",
       "                128         (0.46932116938359664, 0.017810343956085963)   \n",
       "                1024         (0.5541877369045086, 0.013295531450939643)   \n",
       "normal          32          (0.48385686872515327, 0.032847367676860945)   \n",
       "                128          (0.5346388403363386, 0.024947672838258543)   \n",
       "                1024          (0.597961255488895, 0.014559466055023267)   \n",
       "room_layout     32            (0.6190032797314289, 0.06012084973592932)   \n",
       "                128          (0.7214670652736419, 0.014405493707529379)   \n",
       "                1024         (0.7814846922465093, 0.007260365567616939)   \n",
       "segmentsemantic 32           (0.6292784087356239, 0.023335146771729813)   \n",
       "                128          (0.6944363287352295, 0.012964961688379559)   \n",
       "                1024          (0.7538201468004067, 0.00758376566663289)   \n",
       "\n",
       "                                                ZCP + Onehot + F&P + PE  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32            (0.49342264584760254, 0.0460137666200762)   \n",
       "                128          (0.5682644311396872, 0.021306836129126313)   \n",
       "                1024         (0.6124321350195674, 0.007327774415383523)   \n",
       "class_object    32           (0.43195905753680397, 0.05529603330344233)   \n",
       "                128          (0.5546652523823858, 0.016864825891738905)   \n",
       "                1024         (0.6276470222479963, 0.009302832065194456)   \n",
       "class_scene     32           (0.5334327555546071, 0.029681464868250088)   \n",
       "                128           (0.639498464251877, 0.020473841625982907)   \n",
       "                1024         (0.7417887741876835, 0.009258309235851643)   \n",
       "jigsaw          32            (0.3926947645621249, 0.03593172666872009)   \n",
       "                128         (0.47319557906762677, 0.017567802441177602)   \n",
       "                1024         (0.5603151337679675, 0.013098304344293047)   \n",
       "normal          32          (0.48460839600797906, 0.034309956060331784)   \n",
       "                128          (0.5389655126681832, 0.026336888461460307)   \n",
       "                1024         (0.6051229600886052, 0.014073482889382864)   \n",
       "room_layout     32             (0.608976095110317, 0.06740619535812066)   \n",
       "                128          (0.7224286439903009, 0.014310157368571434)   \n",
       "                1024         (0.7828103085834336, 0.007368187530498692)   \n",
       "segmentsemantic 32            (0.6439872554061616, 0.03039597374639378)   \n",
       "                128          (0.7124823781194742, 0.015139755166673992)   \n",
       "                1024         (0.7751509585510337, 0.006838355809522419)   \n",
       "\n",
       "                                                  ZCP + GRAF + F&P + PE  \\\n",
       "dataset         train_size                                                \n",
       "autoencoder     32             (0.5522630794966668, 0.0327689221462198)   \n",
       "                128          (0.5935291747382364, 0.012355128890622926)   \n",
       "                1024         (0.6330223103201728, 0.007976639393444738)   \n",
       "class_object    32            (0.5025359604078671, 0.04837791651179072)   \n",
       "                128           (0.5982513005708373, 0.01608308268058636)   \n",
       "                1024          (0.6476160083365017, 0.00865959742129311)   \n",
       "class_scene     32            (0.5807618772068895, 0.03937726175203158)   \n",
       "                128          (0.6855349428914443, 0.018378200142275564)   \n",
       "                1024         (0.7669213756693692, 0.007532710986628501)   \n",
       "jigsaw          32          (0.44894547314522143, 0.033795221290027676)   \n",
       "                128          (0.5193602366283976, 0.018620997131067137)   \n",
       "                1024         (0.5901621761108548, 0.011846133118834857)   \n",
       "normal          32           (0.5110197021010497, 0.038297069635865766)   \n",
       "                128           (0.5528535568074132, 0.02438230537002612)   \n",
       "                1024          (0.618072805343052, 0.011676727267507044)   \n",
       "room_layout     32           (0.6592657074393751, 0.047787472356045115)   \n",
       "                128          (0.7454628130122778, 0.016456975284451652)   \n",
       "                1024         (0.7944885103038763, 0.007419901751883567)   \n",
       "segmentsemantic 32            (0.6801256552654428, 0.02897394043617278)   \n",
       "                128          (0.7434961890196964, 0.013824914519730816)   \n",
       "                1024         (0.8034090356896613, 0.006864482881829786)   \n",
       "\n",
       "                                                        Everything + PE  \n",
       "dataset         train_size                                               \n",
       "autoencoder     32            (0.549546420577215, 0.031812394819380654)  \n",
       "                128            (0.593485619438475, 0.01208570702329643)  \n",
       "                1024         (0.6323210578307649, 0.008076338931314356)  \n",
       "class_object    32            (0.5032515623797864, 0.04946788422876641)  \n",
       "                128          (0.5968899350888409, 0.016483690681354166)  \n",
       "                1024          (0.647783530674144, 0.008773801937641717)  \n",
       "class_scene     32            (0.5834022947900864, 0.03768671293738099)  \n",
       "                128            (0.685654402869865, 0.01885520253441516)  \n",
       "                1024        (0.7676583949519105, 0.0075301177908345274)  \n",
       "jigsaw          32             (0.4514841629153789, 0.0328045889148291)  \n",
       "                128          (0.5187654789077182, 0.018834257261198323)  \n",
       "                1024         (0.5906822447636347, 0.011297986416360548)  \n",
       "normal          32              (0.5099238768501747, 0.036489454049932)  \n",
       "                128          (0.5546774561380086, 0.024465072691936984)  \n",
       "                1024         (0.6195273763576978, 0.012020018666184747)  \n",
       "room_layout     32            (0.6617332420541168, 0.04579184113146122)  \n",
       "                128          (0.7454857111728346, 0.016361839347127632)  \n",
       "                1024         (0.7948384467148975, 0.007175565649856891)  \n",
       "segmentsemantic 32            (0.6839799342007915, 0.02884933558804223)  \n",
       "                128          (0.7470673111344119, 0.015078729833010401)  \n",
       "                1024          (0.8095243458987124, 0.00629157246076777)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table = pd.merge(table_mean, table_std, left_index=True, right_index=True, suffixes=('_mean', '_std'))\n",
    "\n",
    "def pm_formatter(x):\n",
    "    return f'${x[0]:.2f}' + '^{' + f'{x[1]:.2f}' + '}$'\n",
    "\n",
    "table_str = pd.DataFrame()\n",
    "for c in cols:\n",
    "    table_str[c] = table[[f'{c}_mean', f'{c}_std']].apply(tuple, axis=1)\n",
    "table_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\small\n",
      "\\addtolength{\\tabcolsep}{-0.2em}\n",
      "\\centering\n",
      "\\caption{Results on  different tasks on the Trans-NAS-Bench-101 micro benchmark. Average and standard deviation of Kendall's tau over 50 independent runs.}\n",
      "\\vskip 0.15in\n",
      "\\begin{tabular}{r|ccc|ccc|ccc}\n",
      "dataset & \\multicolumn{3}{|c}{autoencoder} & \\multicolumn{3}{|c}{class\\_object} & \\multicolumn{3}{|c}{class\\_scene} \\\\\n",
      "train\\_size & 32 & 128 & 1024 & 32 & 128 & 1024 & 32 & 128 & 1024 \\\\\n",
      "\\hline\n",
      "Onehot & $0.42^{0.06}$ & $0.55^{0.02}$ & $0.60^{0.01}$ & $0.23^{0.06}$ & $0.41^{0.04}$ & $0.61^{0.01}$ & $0.29^{0.06}$ & $0.51^{0.04}$ & $0.72^{0.01}$ \\\\\n",
      "GRAF & $0.56^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & \\boldmath $0.53^{0.03}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.58^{0.03}$ & $0.67^{0.01}$ & $0.75^{0.01}$ \\\\\n",
      "Onehot + F\\&P & $0.39^{0.07}$ & $0.54^{0.03}$ & $0.60^{0.01}$ & $0.46^{0.05}$ & $0.55^{0.02}$ & $0.62^{0.01}$ & $0.51^{0.03}$ & $0.61^{0.02}$ & $0.72^{0.01}$ \\\\\n",
      "GRAF + Onehot & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & $0.53^{0.03}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.58^{0.03}$ & $0.67^{0.02}$ & $0.75^{0.01}$ \\\\\n",
      "GRAF + F\\&P & \\boldmath $0.56^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & $0.53^{0.04}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.58^{0.03}$ & $0.67^{0.01}$ & $0.75^{0.01}$ \\\\\n",
      "GRAF + Onehot + F\\&P & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & $0.52^{0.04}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.57^{0.03}$ & $0.67^{0.02}$ & $0.75^{0.01}$ \\\\\n",
      "\\hline\n",
      "ZCP + F\\&P & $0.46^{0.04}$ & $0.52^{0.02}$ & $0.56^{0.01}$ & $0.41^{0.05}$ & $0.53^{0.01}$ & $0.60^{0.01}$ & $0.52^{0.03}$ & $0.60^{0.02}$ & $0.71^{0.01}$ \\\\\n",
      "ZCP + Onehot + F\\&P & $0.48^{0.04}$ & $0.55^{0.02}$ & $0.60^{0.01}$ & $0.43^{0.06}$ & $0.55^{0.02}$ & $0.62^{0.01}$ & $0.53^{0.03}$ & $0.64^{0.02}$ & $0.74^{0.01}$ \\\\\n",
      "ZCP + GRAF + F\\&P & $0.55^{0.03}$ & $0.60^{0.01}$ & \\boldmath $0.63^{0.01}$ & $0.50^{0.05}$ & $0.60^{0.02}$ & $0.65^{0.01}$ & $0.58^{0.04}$ & $0.69^{0.02}$ & $0.77^{0.01}$ \\\\\n",
      "Everything & $0.55^{0.03}$ & \\boldmath $0.60^{0.01}$ & $0.63^{0.01}$ & $0.51^{0.05}$ & $0.60^{0.02}$ & $0.65^{0.01}$ & \\boldmath $0.59^{0.04}$ & \\boldmath $0.69^{0.02}$ & $0.77^{0.01}$ \\\\\n",
      "Onehot + PE & $0.47^{0.06}$ & $0.54^{0.02}$ & $0.60^{0.01}$ & $0.23^{0.06}$ & $0.35^{0.02}$ & $0.60^{0.01}$ & $0.30^{0.04}$ & $0.45^{0.02}$ & $0.72^{0.01}$ \\\\\n",
      "GRAF + PE & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.62^{0.01}$ & $0.53^{0.04}$ & \\boldmath $0.60^{0.01}$ & $0.64^{0.01}$ & $0.57^{0.03}$ & $0.67^{0.02}$ & $0.75^{0.01}$ \\\\\n",
      "Onehot + F\\&P + PE & $0.45^{0.07}$ & $0.56^{0.02}$ & $0.61^{0.01}$ & $0.45^{0.06}$ & $0.55^{0.02}$ & $0.62^{0.01}$ & $0.51^{0.04}$ & $0.60^{0.02}$ & $0.73^{0.01}$ \\\\\n",
      "GRAF + Onehot + PE & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.62^{0.01}$ & $0.52^{0.04}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.57^{0.04}$ & $0.67^{0.02}$ & $0.75^{0.01}$ \\\\\n",
      "GRAF + F\\&P + PE & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & $0.53^{0.04}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.58^{0.03}$ & $0.67^{0.02}$ & $0.75^{0.01}$ \\\\\n",
      "GRAF + Onehot + F\\&P + PE & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.62^{0.01}$ & $0.53^{0.04}$ & $0.60^{0.01}$ & $0.64^{0.01}$ & $0.57^{0.04}$ & $0.67^{0.02}$ & $0.76^{0.01}$ \\\\\n",
      "ZCP + F\\&P + PE & $0.50^{0.04}$ & $0.57^{0.02}$ & $0.61^{0.01}$ & $0.43^{0.05}$ & $0.55^{0.02}$ & $0.62^{0.01}$ & $0.53^{0.03}$ & $0.63^{0.02}$ & $0.73^{0.01}$ \\\\\n",
      "ZCP + Onehot + F\\&P + PE & $0.49^{0.05}$ & $0.57^{0.02}$ & $0.61^{0.01}$ & $0.43^{0.06}$ & $0.55^{0.02}$ & $0.63^{0.01}$ & $0.53^{0.03}$ & $0.64^{0.02}$ & $0.74^{0.01}$ \\\\\n",
      "ZCP + GRAF + F\\&P + PE & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & $0.50^{0.05}$ & $0.60^{0.02}$ & $0.65^{0.01}$ & $0.58^{0.04}$ & $0.69^{0.02}$ & $0.77^{0.01}$ \\\\\n",
      "Everything + PE & $0.55^{0.03}$ & $0.59^{0.01}$ & $0.63^{0.01}$ & $0.50^{0.05}$ & $0.60^{0.02}$ & \\boldmath $0.65^{0.01}$ & $0.58^{0.04}$ & $0.69^{0.02}$ & \\boldmath $0.77^{0.01}$ \\\\\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show = sorted([ix for ix in table.index], key=lambda x: (x[0], int(x[1])))\n",
    "esc_labels = [l.replace('F&P', 'F\\&P') for l in cols]\n",
    "table_str.columns=esc_labels\n",
    "table_str = table_str.loc[show].T\n",
    "\n",
    "#columns are now different targets and different train size, rows are different feature sets\n",
    "\n",
    "# here, you can specify which targets will be shown in the table - up to 3 targets fit on the page in ICML pdf\n",
    "# the string is part of the table caption (see the `start` variable below for the whole caption)\n",
    "# it is possible to generate multiple tables at once, if you specify multiple tuples of (caption/targets)\n",
    "# if you change the number of targets, you also should change the `column_format` in the longest line below, or fix it manually, after the tables are generated)\n",
    "c_sets = [(' different tasks on the Trans-NAS-Bench-101 micro benchmark', ['autoencoder', 'class_object', 'class_scene'])]\n",
    "\n",
    "start=\"\"\"\\\\begin{table}\n",
    "\\\\small\n",
    "\\\\addtolength{\\\\tabcolsep}{-0.2em}\n",
    "\\\\centering\n",
    "\\\\caption{Results on %s. Average and standard deviation of Kendall's tau over 50 independent runs.}\n",
    "\\\\vskip 0.15in\"\"\"\n",
    "\n",
    "end=\"\"\"\\\\end{table}\"\"\"\n",
    "\n",
    "# here, it is possible to specify, the position of horizontal lines in the table (numbers are indices of lines after which a hline will be added, only lines with values, without header, count)\n",
    "hlines = [0,6]\n",
    "\n",
    "#the tables are also saved to a file - you can change the filename here\n",
    "with open('result_tables.tex', 'w') as f:\n",
    "    for task, columns in c_sets:\n",
    "        out = table_str[columns].style.highlight_max(axis=0, props='font-weight:bold').format(formatter=pm_formatter).to_latex(convert_css=True, column_format='r|ccc|ccc|ccc', multicol_align='|c')\n",
    "        out = out.replace('_', '\\_').replace('\\\\bfseries', '\\\\boldmath').replace(\"Features\", \"GRAF\")\n",
    "        out = out.splitlines()\n",
    "        for (i,h) in enumerate(hlines):\n",
    "            out[h+3+i:h+3+i] = ['\\\\hline']\n",
    "        f.write(start % task)\n",
    "        print(start % task)\n",
    "        f.write('\\n'.join(out))\n",
    "        print('\\n'.join(out))\n",
    "        f.write(end)\n",
    "        print(end)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "plt.rcParams.update({'text.usetex': True, 'font.family': 'serif', 'text.latex.preamble': '\\\\usepackage{times} ', 'figure.figsize': (3.25, 2.0086104634371584), 'figure.constrained_layout.use': True, 'figure.autolayout': False, 'savefig.bbox': 'tight', 'savefig.pad_inches': 0.015, 'font.size': 10, 'axes.labelsize': 10, 'legend.fontsize': 6, 'xtick.labelsize': 8, 'ytick.labelsize': 8, 'axes.titlesize': 8})\n",
    "\n",
    "#O = Onehot, S = GRAF, F = flops+params, P = ZCP, E = path encoding\n",
    "def_labels = ['O', 'S', 'OF', 'SO', 'SF', 'SOF', 'PF', 'POF', 'PSF', 'PSOF']\n",
    "\n",
    "def plot_experiments(all_data, targets, train_sizes, file_name=None, labels=def_labels):\n",
    "    fig_width = 487.8225/72.27 # \\the\\textsize / points per inch\n",
    "    n_ts = len(train_sizes)\n",
    "    n_tg = len(targets)\n",
    "    \n",
    "    plt.subplots(n_ts, n_tg, sharey=True, figsize=(fig_width, n_ts*2))\n",
    "\n",
    "    for (si,ts) in enumerate(train_sizes):\n",
    "        for (ti,target) in enumerate(targets):\n",
    "\n",
    "            plt_data = all_data[target][ts]\n",
    "            cols = [c for c in labels if c in plt_data.columns]\n",
    "            plt_data = plt_data[cols]\n",
    "            cols = [features_str[f].replace('&', '\\&') for f in cols]\n",
    "            plt_data.columns=cols\n",
    "\n",
    "            plt.subplot(n_ts, n_tg, n_tg*si+ti+1)\n",
    "            sns.boxplot(plt_data, orient='h', linecolor=\"#137\", palette=[sns.color_palette()[0] if 'GRAF' in c else sns.color_palette()[1] for c in cols])\n",
    "            if si == 0:\n",
    "                plt.title(f'{target}')\n",
    "            if ti == 0:\n",
    "                plt.ylabel(f'Training size: {ts}')\n",
    "    \n",
    "    if file_name:\n",
    "        plt.savefig(f'{file_name}.pdf')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All targets: ['autoencoder', 'class_object', 'class_scene', 'jigsaw', 'normal', 'room_layout', 'segmentsemantic']\n",
      "All train_sizes: [32, 128, 1024]\n"
     ]
    }
   ],
   "source": [
    "print(\"All targets:\", list(all_data.columns.get_level_values(0).unique()))\n",
    "print(\"All train_sizes:\", list(all_data.columns.get_level_values(1).unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 486x432 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_experiments(all_data, ['autoencoder', 'class_object', 'class_scene', 'jigsaw'], [32, 128, 1024], file_name='tnb101-micro-1.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
